{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b732e7e",
   "metadata": {},
   "source": [
    "# Instalar dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b3f9063f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' %pip install openai python-dotenv gradio \\n%pip install --upgrade diffusers transformers accelerate scipy safetensors\\n%pip install --upgrade huggingface_hub[hf_xet]  # Incluye hf_xet para mejor rendimiento\\n%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # Para GPU CUDA 12.1\\n%pip install Pillow  # Para manipulaci√≥n de im√°genes\\n%pip install deep-translator  # Para traducir prompts autom√°ticamente '"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instalar dependencias - ejecutar si es la primera vez\n",
    "\"\"\" %pip install openai python-dotenv gradio \n",
    "%pip install --upgrade diffusers transformers accelerate scipy safetensors\n",
    "%pip install --upgrade huggingface_hub[hf_xet]  # Incluye hf_xet para mejor rendimiento\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # Para GPU CUDA 12.1\n",
    "%pip install Pillow  # Para manipulaci√≥n de im√°genes\n",
    "%pip install deep-translator  # Para traducir prompts autom√°ticamente \"\"\"\n",
    "\n",
    "# Si hay errores de compatibilidad, intenta:\n",
    "#pip install diffusers transformers --force-reinstall\n",
    "#pip install diffusers==0.21.4 transformers==4.35.2 --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2d56ed",
   "metadata": {},
   "source": [
    "# Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3e63fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from PIL import Image, ImageDraw\n",
    "import requests\n",
    "from requests.exceptions import Timeout, ConnectionError, RequestException\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import InferenceClient\n",
    "import torch \n",
    "from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fc9fce",
   "metadata": {},
   "source": [
    "# Definir las variables de las API KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6e549807",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "open_router_api_key = os.getenv('OPEN_ROUTER_API_KEY')\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "hf_token = os.getenv('HF_TOKEN')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2349589",
   "metadata": {},
   "source": [
    "### Modelos no disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dd0cc46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN configurado correctamente.\n"
     ]
    }
   ],
   "source": [
    "if not hf_token:\n",
    "    print(\"‚ö†Ô∏èAdvertencia: HF_TOKEN no encontrado. Helsinki no funcionar√°.\")\n",
    "else:\n",
    "    print(\"HF_TOKEN configurado correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7008d4fa",
   "metadata": {},
   "source": [
    "# Conectar los distintos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5934f57d",
   "metadata": {},
   "source": [
    "## Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6a1bf513",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model = dict(model=OpenAI(api_key=gemini_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"), model_name=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f54819",
   "metadata": {},
   "source": [
    "## Helsinki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0f9e11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "helsinki_client = InferenceClient(\n",
    "    api_key=os.getenv(\"HF_TOKEN\"), \n",
    ")\n",
    "\n",
    "class HelsinkiModel:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "        \n",
    "    def translate(self, text):\n",
    "        try:\n",
    "            result = self.client.translation(text, model=self.model_name)\n",
    "            \n",
    "            if isinstance(result, list) and len(result) > 0:\n",
    "                return result[0].get(\"translation_text\", result[0])\n",
    "            elif isinstance(result, dict):\n",
    "                return result.get(\"translation_text\", str(result))\n",
    "            else:\n",
    "                return str(result)\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Error en traducci√≥n: {str(e)}\"\n",
    "    \n",
    "    def chat_completions_create(self, model, messages):\n",
    "        try:\n",
    "            user_message = \"\"\n",
    "            for msg in reversed(messages):\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    user_message = msg[\"content\"]\n",
    "                    break\n",
    "            \n",
    "            translation = self.translate(user_message)\n",
    "            \n",
    "            class Choice:\n",
    "                def __init__(self, translation):\n",
    "                    self.message = type('obj', (object,), {'content': translation})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, translation):\n",
    "                    self.choices = [Choice(translation)]\n",
    "            \n",
    "            return Response(translation)\n",
    "        except Exception as e:\n",
    "            class Choice:\n",
    "                def __init__(self, error):\n",
    "                    self.message = type('obj', (object,), {'content': f\"Error Helsinki: {error}\"})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, error):\n",
    "                    self.choices = [Choice(error)]\n",
    "            \n",
    "            return Response(str(e))\n",
    "\n",
    "helsinki_model_instance = HelsinkiModel(helsinki_client)\n",
    "\n",
    "helsinki_model = dict(\n",
    "    model=helsinki_model_instance, \n",
    "    model_name=\"Helsinki-NLP/opus-mt-en-es\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1b89d",
   "metadata": {},
   "source": [
    "## Runwayml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "999f25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunwaymlModel:\n",
    "    def __init__(self):\n",
    "        self.model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "        self.pipe = None\n",
    "        self.model_name = \"runwayml/stable-diffusion-v1-5\"\n",
    "        self.initialized = False\n",
    "        self.use_placeholder = False\n",
    "        \n",
    "    def initialize_model(self):\n",
    "        if self.initialized:\n",
    "            return\n",
    "            \n",
    "        print(\"Intentando inicializar Stable Diffusion...\")\n",
    "        \n",
    "        try:\n",
    "            self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "                self.model_id,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                safety_checker=None,\n",
    "                requires_safety_checker=False,\n",
    "                use_safetensors=True\n",
    "            )\n",
    "            \n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            print(f\"Usando dispositivo: {device}\")\n",
    "            \n",
    "            self.pipe = self.pipe.to(device)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                self.pipe.enable_attention_slicing()\n",
    "                try:\n",
    "                    self.pipe.enable_model_cpu_offload()\n",
    "                except:\n",
    "                    print(\"‚ö†Ô∏èCPU offload no disponible\")\n",
    "            \n",
    "            print(\"Stable Diffusion inicializado correctamente\")\n",
    "            self.use_placeholder = False\n",
    "            self.initialized = True\n",
    "            \n",
    "        except ImportError as e:\n",
    "            self.use_placeholder = True\n",
    "            self.pipe = \"placeholder\"\n",
    "            self.initialized = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.use_placeholder = True\n",
    "            self.pipe = \"placeholder\"\n",
    "            self.initialized = True\n",
    "        \n",
    "    def generate_image(self, prompt, negative_prompt=\"blurry, low quality, distorted\", num_inference_steps=20, guidance_scale=7.5):\n",
    "        try:\n",
    "            if not self.initialized:\n",
    "                self.initialize_model()\n",
    "            \n",
    "            if self.use_placeholder or self.pipe == \"placeholder\":\n",
    "                return self._generate_placeholder(prompt)\n",
    "            else:\n",
    "                return self._generate_with_diffusion(prompt, negative_prompt, num_inference_steps, guidance_scale)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error en generaci√≥n, usando placeholder: {e}\")\n",
    "            return self._generate_placeholder(prompt)\n",
    "    \n",
    "    def _generate_with_diffusion(self, prompt, negative_prompt, num_inference_steps, guidance_scale):\n",
    "        try:\n",
    "            import torch            \n",
    "            generation_kwargs = {\n",
    "                \"prompt\": prompt,\n",
    "                \"negative_prompt\": negative_prompt,\n",
    "                \"num_inference_steps\": num_inference_steps,\n",
    "                \"guidance_scale\": guidance_scale,\n",
    "                \"height\": 512,\n",
    "                \"width\": 512,\n",
    "            }\n",
    "            \n",
    "            # Generar imagen\n",
    "            with torch.autocast(\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "                result = self.pipe(**generation_kwargs)\n",
    "                image = result.images[0]\n",
    "            \n",
    "            # Convertir a base64\n",
    "            buffer = io.BytesIO()\n",
    "            image.save(buffer, format='PNG')\n",
    "            img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "            \n",
    "            # Guardar imagen\n",
    "            timestamp = int(time.time())\n",
    "            os.makedirs(\"images\", exist_ok=True)\n",
    "            image_filename = f\"images/stable_diffusion_{timestamp}.png\"\n",
    "            image.save(image_filename)\n",
    "            \n",
    "            print(f\"Imagen generada con Stable Diffusion: {image_filename}\")\n",
    "            return image, img_base64, image_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error con Stable Diffusion, usando placeholder: {e}\")\n",
    "            return self._generate_placeholder(prompt)\n",
    "    \n",
    "    def _generate_placeholder(self, prompt):\n",
    "        \"\"\"Genera una imagen placeholder visual\"\"\"\n",
    "        try:\n",
    "            import hashlib\n",
    "            hash_value = int(hashlib.md5(prompt.encode()).hexdigest()[:6], 16)\n",
    "            \n",
    "            r = (hash_value >> 16) & 255\n",
    "            g = (hash_value >> 8) & 255\n",
    "            b = hash_value & 255\n",
    "            \n",
    "            # Asegurar colores visibles\n",
    "            r = max(50, min(200, r))\n",
    "            g = max(50, min(200, g))\n",
    "            b = max(50, min(200, b))\n",
    "            \n",
    "            img = Image.new('RGB', (512, 512), color=(r, g, b))\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            \n",
    "            try:\n",
    "                words = prompt.split()[:4]\n",
    "                text = \" \".join(words)\n",
    "                \n",
    "                bbox = draw.textbbox((0, 0), text)\n",
    "                text_width = bbox[2] - bbox[0]\n",
    "                text_height = bbox[3] - bbox[1]\n",
    "                \n",
    "                x = (512 - text_width) // 2\n",
    "                y = (512 - text_height) // 2\n",
    "                \n",
    "                for dx in [-1, 0, 1]:\n",
    "                    for dy in [-1, 0, 1]:\n",
    "                        if dx != 0 or dy != 0:\n",
    "                            draw.text((x + dx, y + dy), text, fill=(0, 0, 0))\n",
    "                \n",
    "                draw.text((x, y), text, fill=(255, 255, 255))\n",
    "                \n",
    "                placeholder_text = \"PLACEHOLDER\"\n",
    "                bbox2 = draw.textbbox((0, 0), placeholder_text)\n",
    "                text_width2 = bbox2[2] - bbox2[0]\n",
    "                x2 = (512 - text_width2) // 2\n",
    "                y2 = 450\n",
    "                \n",
    "                draw.text((x2, y2), placeholder_text, fill=(200, 200, 200))\n",
    "                \n",
    "            except Exception as text_error:\n",
    "                print(f\"Error con texto: {text_error}\")\n",
    "                draw.ellipse([200, 200, 312, 312], fill=(255, 255, 255))\n",
    "            \n",
    "            buffer = io.BytesIO()\n",
    "            img.save(buffer, format='PNG')\n",
    "            img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "            \n",
    "            os.makedirs(\"images\", exist_ok=True)\n",
    "            image_filename = f\"images/placeholder_image_{timestamp}.png\"\n",
    "            img.save(image_filename)\n",
    "            \n",
    "            return img, img_base64, image_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generando imagen placeholder: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def chat_completions_create(self, messages):\n",
    "        \"\"\"Interfaz compatible con OpenAI para generaci√≥n de im√°genes\"\"\"\n",
    "        try:\n",
    "            user_message = \"\"\n",
    "            for msg in reversed(messages):\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    user_message = msg[\"content\"]\n",
    "                    break\n",
    "            \n",
    "            if not user_message:\n",
    "                raise Exception(\"No se encontr√≥ prompt para generar imagen\")\n",
    "            \n",
    "            image, img_base64, filename = self.generate_image(user_message)\n",
    "            \n",
    "            if self.use_placeholder or self.pipe == \"placeholder\":\n",
    "                response_content = f\"![Imagen generada](data:image/png;base64,{img_base64})\\n\"\n",
    "            else:\n",
    "                response_content = f\"![Imagen generada](data:image/png;base64,{img_base64})\\n\"\n",
    "            \n",
    "            class Choice:\n",
    "                def __init__(self, content):\n",
    "                    self.message = type('obj', (object,), {'content': content})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, content):\n",
    "                    self.choices = [Choice(content)]\n",
    "            \n",
    "            return Response(response_content)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"**Error generando imagen:** {str(e)}\"\n",
    "            print(error_msg)\n",
    "            \n",
    "            class Choice:\n",
    "                def __init__(self, error):\n",
    "                    self.message = type('obj', (object,), {'content': error})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, error):\n",
    "                    self.choices = [Choice(error)]\n",
    "            \n",
    "            return Response(error_msg)\n",
    "\n",
    "runwayml_model_instance = RunwaymlModel()\n",
    "\n",
    "runwayml_model = dict(\n",
    "    model=runwayml_model_instance, \n",
    "    model_name=\"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca0c5d",
   "metadata": {},
   "source": [
    "### stabilityai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f80257e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo gratuito de Hugging Face para generaci√≥n de im√°genes\n",
    "class HuggingFaceImageModel:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.model_name = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "        \n",
    "    def generate_image_hf(self, prompt):\n",
    "        try:\n",
    "            # Usar el cliente de Hugging Face para generar imagen\n",
    "            image = self.client.text_to_image(\n",
    "                prompt=prompt,\n",
    "                model=self.model_name\n",
    "            )\n",
    "            \n",
    "            # La imagen ya es un objeto PIL, no necesita conversi√≥n desde bytes\n",
    "            # Convertir a base64\n",
    "            buffer = io.BytesIO()\n",
    "            image.save(buffer, format='PNG')\n",
    "            img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "            \n",
    "            # Guardar imagen\n",
    "            timestamp = int(time.time())\n",
    "            os.makedirs(\"images\", exist_ok=True)\n",
    "            image_filename = f\"images/hf_sdxl_{timestamp}.png\"\n",
    "            image.save(image_filename)\n",
    "            \n",
    "            print(f\"Imagen generada con Hugging Face SDXL: {image_filename}\")\n",
    "            return image, img_base64, image_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error con Hugging Face, generando placeholder: {e}\")\n",
    "            return self._generate_placeholder(prompt)\n",
    "    \n",
    "    def _generate_placeholder(self, prompt):\n",
    "        \"\"\"Genera una imagen placeholder visual\"\"\"\n",
    "        try:\n",
    "            import hashlib\n",
    "            hash_value = int(hashlib.md5(prompt.encode()).hexdigest()[:6], 16)\n",
    "            \n",
    "            r = (hash_value >> 16) & 255\n",
    "            g = (hash_value >> 8) & 255\n",
    "            b = hash_value & 255\n",
    "            \n",
    "            r = max(50, min(200, r))\n",
    "            g = max(50, min(200, g))\n",
    "            b = max(50, min(200, b))\n",
    "            \n",
    "            img = Image.new('RGB', (512, 512), color=(r, g, b))\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            \n",
    "            try:\n",
    "                words = prompt.split()[:4]\n",
    "                text = \" \".join(words)\n",
    "                \n",
    "                bbox = draw.textbbox((0, 0), text)\n",
    "                text_width = bbox[2] - bbox[0]\n",
    "                text_height = bbox[3] - bbox[1]\n",
    "                \n",
    "                x = (512 - text_width) // 2\n",
    "                y = (512 - text_height) // 2\n",
    "                \n",
    "                for dx in [-1, 0, 1]:\n",
    "                    for dy in [-1, 0, 1]:\n",
    "                        if dx != 0 or dy != 0:\n",
    "                            draw.text((x + dx, y + dy), text, fill=(0, 0, 0))\n",
    "                \n",
    "                draw.text((x, y), text, fill=(255, 255, 255))\n",
    "                \n",
    "                placeholder_text = \"HF PLACEHOLDER\"\n",
    "                bbox2 = draw.textbbox((0, 0), placeholder_text)\n",
    "                text_width2 = bbox2[2] - bbox2[0]\n",
    "                x2 = (512 - text_width2) // 2\n",
    "                y2 = 450\n",
    "                \n",
    "                draw.text((x2, y2), placeholder_text, fill=(200, 200, 200))\n",
    "                \n",
    "            except Exception as text_error:\n",
    "                print(f\"Error con texto: {text_error}\")\n",
    "                draw.ellipse([200, 200, 312, 312], fill=(255, 255, 255))\n",
    "            \n",
    "            buffer = io.BytesIO()\n",
    "            img.save(buffer, format='PNG')\n",
    "            img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "            \n",
    "            os.makedirs(\"images\", exist_ok=True)\n",
    "            image_filename = f\"images/placeholder_hf_{timestamp}.png\"\n",
    "            img.save(image_filename)\n",
    "            \n",
    "            return img, img_base64, image_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generando placeholder: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def chat_completions_create(self, messages):\n",
    "        \"\"\"Interfaz compatible con OpenAI para generaci√≥n de im√°genes\"\"\"\n",
    "        try:\n",
    "            user_message = \"\"\n",
    "            for msg in reversed(messages):\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    user_message = msg[\"content\"]\n",
    "                    break\n",
    "            \n",
    "            if not user_message:\n",
    "                raise Exception(\"No se encontr√≥ prompt para generar imagen\")\n",
    "            \n",
    "            image, img_base64, filename = self.generate_image_hf(user_message)\n",
    "            \n",
    "            response_content = f\"![Imagen generada](data:image/png;base64,{img_base64})\\n**Generado con Stable Diffusion XL (Hugging Face)**\"\n",
    "            \n",
    "            class Choice:\n",
    "                def __init__(self, content):\n",
    "                    self.message = type('obj', (object,), {'content': content})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, content):\n",
    "                    self.choices = [Choice(content)]\n",
    "            \n",
    "            return Response(response_content)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"**Error generando imagen:** {str(e)}\"\n",
    "            print(error_msg)\n",
    "            \n",
    "            class Choice:\n",
    "                def __init__(self, error):\n",
    "                    self.message = type('obj', (object,), {'content': error})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, error):\n",
    "                    self.choices = [Choice(error)]\n",
    "            \n",
    "            return Response(error_msg)\n",
    "\n",
    "hf_image_model_instance = HuggingFaceImageModel(helsinki_client)\n",
    "\n",
    "hf_image_model = dict(\n",
    "    model=hf_image_model_instance,\n",
    "    model_name=\"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f01a9",
   "metadata": {},
   "source": [
    "## OpenRouterAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "670cfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_router_model = dict(model=OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=open_router_api_key,\n",
    "), \n",
    "model_name=\"deepseek/deepseek-r1-0528\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8d7ae",
   "metadata": {},
   "source": [
    "## Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1fd904f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_model = dict(model=OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\"), model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cfcfc0",
   "metadata": {},
   "source": [
    "# Crear interfaz de chat con gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c15b1a",
   "metadata": {},
   "source": [
    "## Funcion de chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a9f1a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_data = {\n",
    "    \"total_requests\": 0,\n",
    "    \"successful_requests\": 0,\n",
    "    \"failed_requests\": 0,\n",
    "    \"requests_by_model\": {},\n",
    "    \"requests_by_task\": {},\n",
    "    \"response_times\": [],\n",
    "    \"errors\": [],\n",
    "    \"session_start\": datetime.now().isoformat(),\n",
    "    \"last_request\": None,\n",
    "    \"images_generated\": 0,\n",
    "    \"translations_made\": 0\n",
    "}\n",
    "\n",
    "def log_metric(metric_type, **kwargs):\n",
    "    \"\"\"Registra m√©tricas del sistema\"\"\"\n",
    "    global metrics_data\n",
    "    \n",
    "    try:\n",
    "        current_time = datetime.now().isoformat()\n",
    "        \n",
    "        if metric_type == \"request_start\":\n",
    "            metrics_data[\"total_requests\"] += 1\n",
    "            metrics_data[\"last_request\"] = current_time\n",
    "            \n",
    "            # Contar por modelo\n",
    "            model = kwargs.get(\"model\", \"unknown\")\n",
    "            if model not in metrics_data[\"requests_by_model\"]:\n",
    "                metrics_data[\"requests_by_model\"][model] = 0\n",
    "            metrics_data[\"requests_by_model\"][model] += 1\n",
    "            \n",
    "            # Contar por tarea\n",
    "            task = kwargs.get(\"task\", \"unknown\")\n",
    "            if task not in metrics_data[\"requests_by_task\"]:\n",
    "                metrics_data[\"requests_by_task\"][task] = 0\n",
    "            metrics_data[\"requests_by_task\"][task] += 1\n",
    "            \n",
    "        elif metric_type == \"request_success\":\n",
    "            metrics_data[\"successful_requests\"] += 1\n",
    "            response_time = kwargs.get(\"response_time\", 0)\n",
    "            metrics_data[\"response_times\"].append(response_time)\n",
    "            \n",
    "            # Contar tipos espec√≠ficos\n",
    "            task = kwargs.get(\"task\", \"\")\n",
    "            if task == \"imagenes\":\n",
    "                metrics_data[\"images_generated\"] += 1\n",
    "            elif task == \"traduccion\":\n",
    "                metrics_data[\"translations_made\"] += 1\n",
    "                \n",
    "        elif metric_type == \"request_error\":\n",
    "            metrics_data[\"failed_requests\"] += 1\n",
    "            error_info = {\n",
    "                \"timestamp\": current_time,\n",
    "                \"model\": kwargs.get(\"model\", \"unknown\"),\n",
    "                \"task\": kwargs.get(\"task\", \"unknown\"),\n",
    "                \"error\": kwargs.get(\"error\", \"Unknown error\"),\n",
    "                \"user_message\": kwargs.get(\"user_message\", \"\")[:100]  # Primeros 100 chars\n",
    "            }\n",
    "            metrics_data[\"errors\"].append(error_info)\n",
    "            \n",
    "            # Mantener solo los √∫ltimos 50 errores para no usar demasiada memoria\n",
    "            if len(metrics_data[\"errors\"]) > 50:\n",
    "                metrics_data[\"errors\"] = metrics_data[\"errors\"][-50:]\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error registrando m√©trica: {e}\")\n",
    "\n",
    "def get_metrics_summary():\n",
    "    try:\n",
    "        response_times = metrics_data[\"response_times\"]\n",
    "        if response_times:\n",
    "            avg_response_time = sum(response_times) / len(response_times)\n",
    "            min_response_time = min(response_times)\n",
    "            max_response_time = max(response_times)\n",
    "        else:\n",
    "            avg_response_time = min_response_time = max_response_time = 0\n",
    "        \n",
    "        # Calcular tasa de √©xito\n",
    "        total = metrics_data[\"total_requests\"]\n",
    "        success_rate = (metrics_data[\"successful_requests\"] / total * 100) if total > 0 else 0\n",
    "        \n",
    "        # Tiempo de sesi√≥n\n",
    "        session_start = datetime.fromisoformat(metrics_data[\"session_start\"])\n",
    "        session_duration = datetime.now() - session_start\n",
    "        \n",
    "        # Modelo m√°s usado\n",
    "        model_usage = metrics_data[\"requests_by_model\"]\n",
    "        most_used_model = max(model_usage.items(), key=lambda x: x[1]) if model_usage else (\"Ninguno\", 0)\n",
    "        \n",
    "        # Tarea m√°s usada\n",
    "        task_usage = metrics_data[\"requests_by_task\"]\n",
    "        most_used_task = max(task_usage.items(), key=lambda x: x[1]) if task_usage else (\"Ninguna\", 0)\n",
    "        \n",
    "        summary = f\"\"\"#**M√©tricas del ChatBot**\n",
    "\n",
    "##**Estad√≠sticas Generales**\n",
    "- **Total de solicitudes:** {total}\n",
    "- **Solicitudes exitosas:** {metrics_data[\"successful_requests\"]} ({success_rate:.1f}%)\n",
    "- **Solicitudes fallidas:** {metrics_data[\"failed_requests\"]}\n",
    "- **Im√°genes generadas:** {metrics_data[\"images_generated\"]}\n",
    "- **Traducciones realizadas:** {metrics_data[\"translations_made\"]}\n",
    "\n",
    "##**Tiempos de Respuesta**\n",
    "- **Promedio:** {avg_response_time:.2f}s\n",
    "- **M√≠nimo:** {min_response_time:.2f}s\n",
    "- **M√°ximo:** {max_response_time:.2f}s\n",
    "- **Total de mediciones:** {len(response_times)}\n",
    "\n",
    "##**Uso por Modelo**\n",
    "\"\"\"\n",
    "        \n",
    "        # Agregar uso por modelo\n",
    "        for model, count in sorted(model_usage.items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / total * 100) if total > 0 else 0\n",
    "            summary += f\"- **{model.capitalize()}:** {count} ({percentage:.1f}%)\\n\"\n",
    "        \n",
    "        summary += f\"\\n## üìã **Uso por Tarea**\\n\"\n",
    "        \n",
    "        # Agregar uso por tarea\n",
    "        for task, count in sorted(task_usage.items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / total * 100) if total > 0 else 0\n",
    "            summary += f\"- **{task.capitalize()}:** {count} ({percentage:.1f}%)\\n\"\n",
    "        \n",
    "        summary += f\"\"\"\n",
    "##**Informaci√≥n de Sesi√≥n**\n",
    "- **Inicio de sesi√≥n:** {session_start.strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "- **Duraci√≥n:** {str(session_duration).split('.')[0]}\n",
    "- **√öltima solicitud:** {metrics_data[\"last_request\"] or \"Ninguna\"}\n",
    "\n",
    "##**M√°s Utilizados**\n",
    "- **Modelo m√°s usado:** {most_used_model[0]} ({most_used_model[1]} usos)\n",
    "- **Tarea m√°s usada:** {most_used_task[0]} ({most_used_task[1]} usos)\n",
    "\"\"\"\n",
    "        \n",
    "        # Agregar errores recientes si los hay\n",
    "        recent_errors = metrics_data[\"errors\"][-5:] if metrics_data[\"errors\"] else []\n",
    "        if recent_errors:\n",
    "            summary += \"\\n## ‚ö†Ô∏è **Errores Recientes (√öltimos 5)**\\n\"\n",
    "            for i, error in enumerate(recent_errors, 1):\n",
    "                timestamp = error[\"timestamp\"][:19].replace(\"T\", \" \")  # Formato legible\n",
    "                summary += f\"{i}. **[{timestamp}]** {error['model']} - {error['task']}: {error['error'][:100]}...\\n\"\n",
    "        \n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"**Error generando m√©tricas:** {str(e)}\"\n",
    "\n",
    "def reset_metrics():\n",
    "    global metrics_data\n",
    "    metrics_data = {\n",
    "        \"total_requests\": 0,\n",
    "        \"successful_requests\": 0,\n",
    "        \"failed_requests\": 0,\n",
    "        \"requests_by_model\": {},\n",
    "        \"requests_by_task\": {},\n",
    "        \"response_times\": [],\n",
    "        \"errors\": [],\n",
    "        \"session_start\": datetime.now().isoformat(),\n",
    "        \"last_request\": None,\n",
    "        \"images_generated\": 0,\n",
    "        \"translations_made\": 0\n",
    "    }\n",
    "    return \"**M√©tricas reiniciadas correctamente**\"\n",
    "\n",
    "def obtener_valor(dict: dict, key):\n",
    "    return dict.get(key, \"Clave no encontrada\")\n",
    "\n",
    "models = dict(\n",
    "    gemini=gemini_model,\n",
    "    open_router=open_router_model,\n",
    "    groq=groq_model,\n",
    "    helsinki=helsinki_model,  \n",
    "    runwayml=runwayml_model,\n",
    "    hf_image=hf_image_model\n",
    ")\n",
    "\n",
    "tasks_config = {\n",
    "    \"traduccion\": {\n",
    "        \"models\": [\"gemini\", \"helsinki\"], \n",
    "        \"description\": \"Traducir texto entre idiomas (Helsinki: solo ingl√©s ‚Üí espa√±ol)\"\n",
    "    },\n",
    "    \"resumen\": {\n",
    "        \"models\": [\"groq\", \"open_router\"],\n",
    "        \"description\": \"Resumir texto o documentos\"\n",
    "    },\n",
    "    \"imagenes\": {\n",
    "        \"models\": [\"runwayml\", \"stabilityai\"],  \n",
    "        \"description\": \"Generar im√°genes usando Stable Diffusion local o Hugging Face SDXL (gratis)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_available_models(task):\n",
    "    \"\"\"Obtiene los modelos disponibles para una tarea espec√≠fica\"\"\"\n",
    "    return tasks_config.get(task, {}).get(\"models\", [])\n",
    "\n",
    "def validate_input(message):\n",
    "    \"\"\"Valida el mensaje de entrada\"\"\"\n",
    "    if not message:\n",
    "        return False, \"**Error**: No puedes enviar un mensaje vac√≠o. Por favor, escribe algo.\"\n",
    "    \n",
    "    if len(message.strip()) == 0:\n",
    "        return False, \"**Error**: El mensaje solo contiene espacios en blanco. Por favor, escribe un mensaje v√°lido.\"\n",
    "    \n",
    "    if len(message) > 8000:\n",
    "        return False, \"**Error**: El mensaje es demasiado largo (m√°ximo 8000 caracteres). Por favor, ac√≥rtalo.\"\n",
    "    \n",
    "    return True, None\n",
    "\n",
    "def format_inference_time(seconds):\n",
    "    if seconds < 1:\n",
    "        return f\"{seconds*1000:.0f}ms\"\n",
    "    elif seconds < 60:\n",
    "        return f\"{seconds:.2f}s\"\n",
    "    else:\n",
    "        minutes = int(seconds // 60)\n",
    "        remaining_seconds = seconds % 60\n",
    "        return f\"{minutes}m {remaining_seconds:.1f}s\"\n",
    "\n",
    "def create_initial_chat():\n",
    "    chat_id = f\"Chat-{str(uuid.uuid4())[:8]}\"\n",
    "    return chat_id, []\n",
    "\n",
    "def chat(message, history, model, task, chats_state, current_chat_id):    \n",
    "    total_start_time = time.time()\n",
    "    log_metric(\"request_start\", model=model, task=task, user_message=message)\n",
    "    is_valid, error_msg = validate_input(message)\n",
    "    if not is_valid:\n",
    "        log_metric(\"request_error\", model=model, task=task, error=\"Invalid input\", user_message=message)\n",
    "        return error_msg\n",
    "    try:\n",
    "        print(f\"Tarea seleccionada: {task}\")\n",
    "        print(f\"Modelo seleccionado: {model}\")\n",
    "        print(f\"Chat actual: {current_chat_id}\")\n",
    "        if task not in tasks_config:\n",
    "            error = f\"Tarea '{task}' no reconocida\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"‚ùå **Error**: {error}. Tareas disponibles: {', '.join(tasks_config.keys())}\"\n",
    "        \n",
    "        available_models = get_available_models(task)\n",
    "        if not available_models:\n",
    "            error = f\"No hay modelos disponibles para la tarea '{task}'\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"‚ùå **Error**: {error}.\"\n",
    "        \n",
    "        if model not in available_models:\n",
    "            error = f\"El modelo '{model}' no est√° disponible para la tarea '{task}'\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"‚ùå **Error**: {error}. Modelos disponibles: {', '.join(available_models)}\"\n",
    "        \n",
    "        if model not in models:\n",
    "            error = f\"El modelo '{model}' no est√° configurado en el sistema\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"‚ùå **Error**: {error}.\"\n",
    "        \n",
    "        model_config = models[model]\n",
    "        if not model_config or obtener_valor(model_config, \"model\") == \"Clave no encontrada\":\n",
    "            error = f\"El modelo '{model}' no est√° disponible. Verifica la configuraci√≥n de API keys\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"‚ùå **Error**: {error}.\"\n",
    "\n",
    "        print(f\"modelo: {obtener_valor(model_config, 'model_name')}\")\n",
    "\n",
    "        system_prompt = \"Eres un asistente de IA que responde preguntas y ayuda con tareas.\"\n",
    "        \n",
    "        if task == \"traduccion\":\n",
    "            if model == \"helsinki\":\n",
    "                system_prompt = \"Eres un traductor especializado que traduce √öNICAMENTE de ingl√©s a espa√±ol usando modelos Helsinki-NLP. Solo acepta texto en ingl√©s y lo traduce al espa√±ol.\"\n",
    "            else:\n",
    "                system_prompt = \"Eres un traductor experto. Tu tarea es traducir texto entre diferentes idiomas de manera precisa y natural.\"\n",
    "        elif task == \"resumen\":\n",
    "            system_prompt = \"Eres un asistente especializado en resumir texto. Tu tarea es crear res√∫menes claros y concisos del contenido proporcionado.\"\n",
    "        elif task == \"imagenes\":\n",
    "            if model == \"runwayml\":\n",
    "                system_prompt = \"Eres un generador de im√°genes que usa Stable Diffusion cuando est√° disponible, o genera placeholders visuales cuando hay problemas t√©cnicos. Conviertes descripciones de texto en im√°genes.\"\n",
    "\n",
    "        chat_history = []\n",
    "        if current_chat_id and current_chat_id in chats_state:\n",
    "            chat_history = chats_state[current_chat_id].copy()\n",
    "        \n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        \n",
    "        recent_history = chat_history[-20:] if len(chat_history) > 20 else chat_history\n",
    "        for msg in recent_history:\n",
    "            if isinstance(msg, dict) and \"role\" in msg and \"content\" in msg:\n",
    "                messages.append({\"role\": msg[\"role\"], \"content\": msg[\"content\"]})\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        model_instance = obtener_valor(model_config, \"model\")\n",
    "        response_content = None\n",
    "        \n",
    "        inference_start_time = None\n",
    "        inference_end_time = None\n",
    "        total_attempts = 0\n",
    "        \n",
    "        max_retries = 2\n",
    "        for attempt in range(max_retries + 1):\n",
    "            try:\n",
    "                total_attempts += 1\n",
    "                \n",
    "                inference_start_time = time.time()\n",
    "                if model in [\"helsinki\", \"runwayml\", \"hf_image\"]:\n",
    "                    resp = model_instance.chat_completions_create(\n",
    "                        messages=messages,\n",
    "                    )\n",
    "                else:\n",
    "                    resp = model_instance.chat.completions.create(\n",
    "                        model=obtener_valor(model_config, \"model_name\"),\n",
    "                        messages=messages,\n",
    "                    )\n",
    "\n",
    "                inference_end_time = time.time()\n",
    "\n",
    "                if not resp or not hasattr(resp, 'choices') or len(resp.choices) == 0:\n",
    "                    raise Exception(\"Respuesta vac√≠a del modelo\")\n",
    "                \n",
    "                if not hasattr(resp.choices[0], 'message') or not hasattr(resp.choices[0].message, 'content'):\n",
    "                    raise Exception(\"Formato de respuesta inv√°lido\")\n",
    "                \n",
    "                response_content = resp.choices[0].message.content\n",
    "                \n",
    "                if not response_content or len(response_content.strip()) == 0:\n",
    "                    raise Exception(\"El modelo devolvi√≥ una respuesta vac√≠a\")\n",
    "                \n",
    "                break \n",
    "                \n",
    "            except Timeout:\n",
    "                inference_end_time = time.time()  \n",
    "                if attempt < max_retries:\n",
    "                    print(f\"Timeout en intento {attempt + 1}, reintentando...\")\n",
    "                    time.sleep(2)  \n",
    "                    continue\n",
    "                else:\n",
    "                    error = f\"El modelo '{model}' tard√≥ demasiado en responder\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"‚è±**Error de Timeout**: {error}. Intenta de nuevo en unos momentos.\"\n",
    "            \n",
    "            except ConnectionError:\n",
    "                inference_end_time = time.time()  \n",
    "                if attempt < max_retries:\n",
    "                    print(f\"Error de conexi√≥n en intento {attempt + 1}, reintentando...\")\n",
    "                    time.sleep(2)\n",
    "                    continue\n",
    "                else:\n",
    "                    error = f\"No se pudo conectar con el modelo '{model}'\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"**Error de Conexi√≥n**: {error}. Verifica tu conexi√≥n a internet.\"\n",
    "            \n",
    "            except Exception as api_error:\n",
    "                inference_end_time = time.time()\n",
    "                error_msg = str(api_error).lower()\n",
    "                \n",
    "                if \"rate limit\" in error_msg or \"quota\" in error_msg:\n",
    "                    error = f\"L√≠mite de uso alcanzado para el modelo '{model}'\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"**L√≠mite de Uso**: {error}. Intenta m√°s tarde o cambia de modelo.\"\n",
    "                \n",
    "                elif \"authentication\" in error_msg or \"unauthorized\" in error_msg or \"api key\" in error_msg:\n",
    "                    error = f\"Error de autenticaci√≥n del modelo '{model}'\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"**Error de Autenticaci√≥n**: La API key del modelo '{model}' es inv√°lida o ha expirado. Verifica tu configuraci√≥n.\"\n",
    "                \n",
    "                elif \"not found\" in error_msg or \"404\" in error_msg:\n",
    "                    error = f\"El modelo '{obtener_valor(model_config, 'model_name')}' no est√° disponible\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"**Modelo No Encontrado**: {error} en este momento.\"\n",
    "                \n",
    "                elif \"server error\" in error_msg or \"500\" in error_msg or \"502\" in error_msg or \"503\" in error_msg:\n",
    "                    if attempt < max_retries:\n",
    "                        print(f\"Error del servidor en intento {attempt + 1}, reintentando...\")\n",
    "                        time.sleep(3)\n",
    "                        continue\n",
    "                    else:\n",
    "                        error = f\"El servicio del modelo '{model}' est√° temporalmente no disponible\"\n",
    "                        log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                        return f\"üîß **Error del Servidor**: {error}. Intenta m√°s tarde.\"\n",
    "                \n",
    "                elif \"content policy\" in error_msg or \"safety\" in error_msg:\n",
    "                    error = f\"Contenido bloqueado por las pol√≠ticas de seguridad del modelo '{model}'\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"‚ö†Ô∏è**Contenido Bloqueado**: {error}. Reformula tu pregunta.\"\n",
    "                \n",
    "                else:\n",
    "                    if attempt < max_retries:\n",
    "                        print(f\"Error gen√©rico en intento {attempt + 1}: {api_error}\")\n",
    "                        time.sleep(2)\n",
    "                        continue\n",
    "                    else:\n",
    "                        error = f\"Problema con el modelo '{model}': {str(api_error)[:100]}\"\n",
    "                        log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                        return f\"**Error**: {error}...\"\n",
    "\n",
    "        if not response_content:\n",
    "            error = f\"No se pudo obtener una respuesta v√°lida del modelo '{model}' despu√©s de {max_retries + 1} intentos\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"**Error**: {error}.\"\n",
    "        \n",
    "        total_end_time = time.time()\n",
    "        total_time = total_end_time - total_start_time\n",
    "        \n",
    "        if inference_start_time and inference_end_time:\n",
    "            inference_time = inference_end_time - inference_start_time\n",
    "        else:\n",
    "            inference_time = total_time\n",
    "        \n",
    "        log_metric(\"request_success\", model=model, task=task, response_time=inference_time)\n",
    "        \n",
    "        model_name = obtener_valor(model_config, \"model_name\")\n",
    "        model_display = f\"{model.capitalize()}\"\n",
    "        if model_name and model_name != \"Clave no encontrada\":\n",
    "            model_display += f\" ({model_name})\"\n",
    "        \n",
    "        time_metadata = f\"<small style='color: #888; font-size: 0.85em;'>‚ö° {format_inference_time(inference_time)}\"\n",
    "        \n",
    "        if total_attempts > 1:\n",
    "            time_metadata += f\" ({total_attempts}¬∫ intento)\"\n",
    "        \n",
    "        time_metadata += f\" ‚Ä¢ {model_display}</small>\"\n",
    "        \n",
    "        final_response = response_content + \"\\n\\n\" + time_metadata\n",
    "        \n",
    "        if current_chat_id:\n",
    "            try:\n",
    "                current_history = chats_state.get(current_chat_id, []).copy()\n",
    "                \n",
    "                current_history.extend([\n",
    "                    {\"role\": \"user\", \"content\": message},\n",
    "                    {\"role\": \"assistant\", \"content\": final_response}\n",
    "                ])\n",
    "                \n",
    "                chats_state[current_chat_id] = current_history\n",
    "                \n",
    "                print(f\"Historial guardado en chat {current_chat_id}: {len(current_history)} mensajes\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al guardar en historial: {e}\")        \n",
    "        return final_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        total_end_time = time.time()\n",
    "        total_time = total_end_time - total_start_time\n",
    "        error = f\"Error inesperado: {str(e)[:200]}\"\n",
    "        log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "        print(f\"Error inesperado: {e}\")\n",
    "        return f\"‚ùå **Error Inesperado**: Ocurri√≥ un problema t√©cnico: {str(e)[:200]}... \\n\\n<small style='color: #888;'>‚è±Ô∏è {format_inference_time(total_time)}</small>\"\n",
    "    \n",
    "def new_chat(chats_state: dict):\n",
    "    try:\n",
    "        chat_id = f\"Chat-{str(uuid.uuid4())[:8]}\" \n",
    "        while chat_id in chats_state:\n",
    "            chat_id = f\"Chat-{str(uuid.uuid4())[:8]}\"\n",
    "        chats_state[chat_id] = []\n",
    "        print(f\"Nuevo chat creado: {chat_id}\")\n",
    "        chat_choices = list(chats_state.keys())\n",
    "\n",
    "        return chats_state, chat_id, gr.Dropdown(choices=chat_choices, value=chat_id), []\n",
    "    except Exception as e:\n",
    "        print(f\"Error al crear nuevo chat: {e}\")\n",
    "        return chats_state, \"\", gr.Dropdown(choices=list(chats_state.keys())), []\n",
    "\n",
    "def switch_chat(chats_state, chat_id):\n",
    "    try:\n",
    "        print(f\"Cambiando a chat: {chat_id}\")\n",
    "        \n",
    "        if chat_id and chat_id in chats_state:\n",
    "            chat_history = chats_state[chat_id].copy()\n",
    "            print(f\"Historial del chat {chat_id}: {len(chat_history)} mensajes\")\n",
    "            return chat_history\n",
    "        else:\n",
    "            print(f\"Chat {chat_id} no encontrado o ID vac√≠o\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cambiar chat: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_message(chats_state, chat_id, history):\n",
    "    try:\n",
    "        if chat_id and chat_id in chats_state:\n",
    "            print(f\"save_message llamado para {chat_id} - ignorando para evitar conflictos\")\n",
    "        return chats_state\n",
    "    except Exception as e:\n",
    "        print(f\"Error en save_message: {e}\")\n",
    "        return chats_state\n",
    "\n",
    "def initialize_first_chat():\n",
    "    chat_id, empty_history = create_initial_chat()\n",
    "    initial_state = {chat_id: empty_history}\n",
    "    return initial_state, chat_id, [chat_id], empty_history\n",
    "\n",
    "def update_models_for_task(task):\n",
    "    try:\n",
    "        available_models = get_available_models(task)\n",
    "        if available_models:\n",
    "            return gr.Dropdown(choices=available_models, value=available_models[0])\n",
    "        else:\n",
    "            return gr.Dropdown(choices=[], value=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al actualizar modelos: {e}\")\n",
    "        return gr.Dropdown(choices=[], value=None)\n",
    "\n",
    "def update_chat_list(chats_state):\n",
    "    try:\n",
    "        chat_choices = list(chats_state.keys()) if chats_state else []\n",
    "        return gr.Dropdown(choices=chat_choices)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al actualizar lista de chats: {e}\")\n",
    "        return gr.Dropdown(choices=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206937d",
   "metadata": {},
   "source": [
    "## Creacion de la interfaz del Chat con Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3dc87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarea seleccionada: imagenes\n",
      "Modelo seleccionado: runwayml\n",
      "Chat actual: Chat-21b744a2\n",
      "modelo: runwayml/stable-diffusion-v1-5\n",
      "Intentando inicializar Stable Diffusion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [00:01<00:00,  2.83it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generando imagen placeholder: name 'timestamp' is not defined\n",
      "Error en generaci√≥n, usando placeholder: name 'timestamp' is not defined\n",
      "Error generando imagen placeholder: name 'timestamp' is not defined\n",
      "**Error generando imagen:** name 'timestamp' is not defined\n",
      "Historial guardado en chat Chat-21b744a2: 2 mensajes\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as app:\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## Configuraci√≥n del Chat\")\n",
    "            \n",
    "            task_selector = gr.Dropdown(\n",
    "                choices=list(tasks_config.keys()),\n",
    "                value=\"traduccion\",\n",
    "                label=\"Tipo de tarea\"\n",
    "            )\n",
    "            \n",
    "            model_selector = gr.Dropdown(\n",
    "                choices=get_available_models(\"traduccion\"),\n",
    "                value=\"gemini\" if get_available_models(\"traduccion\") else None,\n",
    "                label=\"Modelo de IA\"\n",
    "            )\n",
    "\n",
    "            initial_state, initial_chat_id, initial_choices, initial_history = initialize_first_chat()\n",
    "\n",
    "            selected_chat = gr.Dropdown(\n",
    "                choices=initial_choices,\n",
    "                value=initial_chat_id,\n",
    "                label=\"Historial de chats\",\n",
    "                interactive=True\n",
    "            )\n",
    "            \n",
    "            new_chat_btn = gr.Button(\"‚ûï Nuevo Chat\", size=\"md\")\n",
    "            \n",
    "            task_info = gr.Markdown(f\"**Descripci√≥n:** {tasks_config['traduccion']['description']}\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                metrics_btn = gr.Button(\"üìä Ver M√©tricas\", size=\"sm\", variant=\"secondary\")\n",
    "                reset_metrics_btn = gr.Button(\"üîÑ Reiniciar M√©tricas\", size=\"sm\", variant=\"secondary\")\n",
    "            \n",
    "            metrics_display = gr.Markdown(\n",
    "                value=\"Haz clic en **Ver M√©tricas** para mostrar las estad√≠sticas de uso.\",\n",
    "                visible=False,\n",
    "                label=\"M√©tricas del Sistema\"\n",
    "            )\n",
    "            \n",
    "            metrics_visible = gr.State(False)\n",
    "            \n",
    "            chat_state = gr.State(initial_state) \n",
    "            current_chat = gr.State(initial_chat_id) \n",
    "\n",
    "        with gr.Column(scale=3):\n",
    "            current_model = gr.State(\"gemini\")\n",
    "            current_task = gr.State(\"traduccion\")\n",
    "\n",
    "            task_selector.change(\n",
    "                fn=lambda task: [\n",
    "                    update_models_for_task(task),\n",
    "                    f\"**Descripci√≥n:** {tasks_config.get(task, {}).get('description', '')}\", \n",
    "                    task,\n",
    "                    get_available_models(task)[0] if get_available_models(task) else None\n",
    "                ],\n",
    "                inputs=task_selector,\n",
    "                outputs=[model_selector, task_info, current_task, current_model]\n",
    "            )\n",
    "\n",
    "            model_selector.change(\n",
    "                fn=lambda model: model,\n",
    "                inputs=model_selector,\n",
    "                outputs=current_model\n",
    "            )\n",
    "\n",
    "            chatbot = gr.Chatbot(type=\"messages\", height=500, value=initial_history)\n",
    "\n",
    "            chat_interface = gr.ChatInterface(\n",
    "                chat, \n",
    "                chatbot=chatbot, \n",
    "                additional_inputs=[current_model, current_task, chat_state, current_chat], \n",
    "                type=\"messages\", \n",
    "                title=\"Chat con Modelos de IA\"\n",
    "            )\n",
    "\n",
    "            def on_chat_change(chats_state, chat_id):                \n",
    "                if not chat_id or chat_id not in chats_state:\n",
    "                    print(f\"Chat {chat_id} no v√°lido\")\n",
    "                    return [], chat_id\n",
    "                chat_history = switch_chat(chats_state, chat_id)                \n",
    "                return chat_history, chat_id\n",
    "\n",
    "            selected_chat.change(\n",
    "                fn=on_chat_change,\n",
    "                inputs=[chat_state, selected_chat],\n",
    "                outputs=[chatbot, current_chat]\n",
    "            )\n",
    "\n",
    "            def on_new_chat(chats_state):                \n",
    "                try:\n",
    "                    new_state, new_chat_id, updated_dropdown, empty_history = new_chat(chats_state)\n",
    "                    \n",
    "                    print(f\"Nuevo chat creado: {new_chat_id}\")\n",
    "                    print(f\"Estado actual: {list(new_state.keys())}\")\n",
    "                    \n",
    "                    return new_state, new_chat_id, updated_dropdown, empty_history\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error al crear nuevo chat: {e}\")\n",
    "                    return chats_state, \"\", gr.Dropdown(choices=list(chats_state.keys())), []\n",
    "\n",
    "            # Funci√≥n para toggle de m√©tricas\n",
    "            def toggle_metrics(is_visible):\n",
    "                if is_visible:\n",
    "                    return False, gr.Markdown(visible=False)\n",
    "                else:\n",
    "                    metrics_content = get_metrics_summary()\n",
    "                    return True, gr.Markdown(value=metrics_content, visible=True)\n",
    "\n",
    "            # Funci√≥n para reiniciar m√©tricas\n",
    "            def reset_metrics_display(is_visible):\n",
    "                result = reset_metrics()\n",
    "                if is_visible:\n",
    "                    return is_visible, gr.Markdown(value=result, visible=True)\n",
    "                else:\n",
    "                    return is_visible, gr.Markdown(visible=False)\n",
    "\n",
    "            new_chat_btn.click(\n",
    "                fn=on_new_chat,\n",
    "                inputs=[chat_state],\n",
    "                outputs=[chat_state, current_chat, selected_chat, chatbot]\n",
    "            )\n",
    "\n",
    "            metrics_btn.click(\n",
    "                fn=toggle_metrics,\n",
    "                inputs=metrics_visible,\n",
    "                outputs=[metrics_visible, metrics_display]\n",
    "            )\n",
    "\n",
    "            reset_metrics_btn.click(\n",
    "                fn=reset_metrics_display,\n",
    "                inputs=metrics_visible,\n",
    "                outputs=[metrics_visible, metrics_display]\n",
    "            )\n",
    "\n",
    "app.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

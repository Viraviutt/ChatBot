{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b732e7e",
   "metadata": {},
   "source": [
    "# Instalar dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3f9063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instalar dependencias - ejecutar si es la primera vez\n",
    "#%pip install openai python-dotenv gradio PyMuPDF docx2txt\n",
    "#%pip install --upgrade diffusers transformers accelerate scipy safetensors\n",
    "#%pip install --upgrade huggingface_hub[hf_xet]  # Incluye hf_xet para mejor rendimiento\n",
    "#%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # Para GPU CUDA 12.1\n",
    "#%pip install Pillow  # Para manipulación de imágenes\n",
    "#%pip install deep-translator  # Para traducir prompts automáticamente\n",
    "#%pip install requests  # Para comunicación con Ollama\n",
    "\n",
    "# Si hay errores de compatibilidad, intenta:\n",
    "#pip install diffusers transformers --force-reinstall\n",
    "#pip install diffusers==0.21.4 transformers==4.35.2 --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2d56ed",
   "metadata": {},
   "source": [
    "# Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e63fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import uuid\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from PIL import Image, ImageDraw\n",
    "import requests\n",
    "from requests.exceptions import Timeout, ConnectionError\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import InferenceClient\n",
    "import torch \n",
    "from diffusers import StableDiffusionPipeline\n",
    "import fitz # PyMuPDF\n",
    "import docx2txt\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fc9fce",
   "metadata": {},
   "source": [
    "# Definir las variables de las API KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e549807",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "open_router_api_key = os.getenv('OPEN_ROUTER_API_KEY')\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "hf_token = os.getenv('HF_TOKEN')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2349589",
   "metadata": {},
   "source": [
    "### Modelos no disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd0cc46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN configurado correctamente.\n"
     ]
    }
   ],
   "source": [
    "if not hf_token:\n",
    "    print(\"⚠️Advertencia: HF_TOKEN no encontrado. Helsinki no funcionará.\")\n",
    "else:\n",
    "    print(\"HF_TOKEN configurado correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7008d4fa",
   "metadata": {},
   "source": [
    "# Conectar los distintos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5934f57d",
   "metadata": {},
   "source": [
    "## Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a1bf513",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model = dict(model=OpenAI(api_key=gemini_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"), model_name=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f54819",
   "metadata": {},
   "source": [
    "## Helsinki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f9e11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "helsinki_client = InferenceClient(\n",
    "    api_key=hf_token,\n",
    ")\n",
    "\n",
    "class HelsinkiModel:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "        \n",
    "    def translate(self, text):\n",
    "        try:\n",
    "            result = self.client.translation(text, model=self.model_name)\n",
    "            \n",
    "            if isinstance(result, list) and len(result) > 0:\n",
    "                return result[0].get(\"translation_text\", result[0])\n",
    "            elif isinstance(result, dict):\n",
    "                return result.get(\"translation_text\", str(result))\n",
    "            else:\n",
    "                return str(result)\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Error en traducción: {str(e)}\"\n",
    "    \n",
    "    def chat_completions_create(self, messages):\n",
    "        try:\n",
    "            user_message = \"\"\n",
    "            for msg in reversed(messages):\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    user_message = msg[\"content\"]\n",
    "                    break\n",
    "            \n",
    "            translation = self.translate(user_message)\n",
    "            \n",
    "            class Choice:\n",
    "                def __init__(self, translation):\n",
    "                    self.message = type('obj', (object,), {'content': translation})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, translation):\n",
    "                    self.choices = [Choice(translation)]\n",
    "            \n",
    "            return Response(translation)\n",
    "        except Exception as e:\n",
    "            class Choice:\n",
    "                def __init__(self, error):\n",
    "                    self.message = type('obj', (object,), {'content': f\"Error Helsinki: {error}\"})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, error):\n",
    "                    self.choices = [Choice(error)]\n",
    "            \n",
    "            return Response(str(e))\n",
    "\n",
    "helsinki_model_instance = HelsinkiModel(helsinki_client)\n",
    "\n",
    "helsinki_model = dict(\n",
    "    model=helsinki_model_instance, \n",
    "    model_name=\"Helsinki-NLP/opus-mt-en-es\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1b89d",
   "metadata": {},
   "source": [
    "## Runwayml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "670cfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunwaymlModel:\n",
    "    def __init__(self):\n",
    "        self.model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "        self.pipe = None\n",
    "        self.model_name = \"runwayml/stable-diffusion-v1-5\"\n",
    "        self.initialized = False\n",
    "        self.use_placeholder = False\n",
    "        \n",
    "    def initialize_model(self):\n",
    "        if self.initialized:\n",
    "            return\n",
    "            \n",
    "        print(\"Intentando inicializar Stable Diffusion...\")\n",
    "        \n",
    "        try:\n",
    "            self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "                self.model_id,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                safety_checker=None,\n",
    "                requires_safety_checker=False,\n",
    "                use_safetensors=True\n",
    "            )\n",
    "            \n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            print(f\"Usando dispositivo: {device}\")\n",
    "            \n",
    "            self.pipe = self.pipe.to(device)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                self.pipe.enable_attention_slicing()\n",
    "                try:\n",
    "                    self.pipe.enable_model_cpu_offload()\n",
    "                except:\n",
    "                    print(\"⚠️CPU offload no disponible\")\n",
    "            \n",
    "            print(\"Stable Diffusion inicializado correctamente\")\n",
    "            self.use_placeholder = False\n",
    "            self.initialized = True\n",
    "            \n",
    "        except ImportError as e:\n",
    "            self.use_placeholder = True\n",
    "            self.pipe = \"placeholder\"\n",
    "            self.initialized = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.use_placeholder = True\n",
    "            self.pipe = \"placeholder\"\n",
    "            self.initialized = True\n",
    "        \n",
    "    def generate_image(self, prompt, negative_prompt=\"blurry, low quality, distorted\", num_inference_steps=20, guidance_scale=7.5):\n",
    "        try:\n",
    "            if not self.initialized:\n",
    "                self.initialize_model()\n",
    "            \n",
    "            if self.use_placeholder or self.pipe == \"placeholder\":\n",
    "                return self._generate_placeholder(prompt)\n",
    "            else:\n",
    "                return self._generate_with_diffusion(prompt, negative_prompt, num_inference_steps, guidance_scale)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error en generación, usando placeholder: {e}\")\n",
    "            return self._generate_placeholder(prompt)\n",
    "    \n",
    "    def _generate_with_diffusion(self, prompt, negative_prompt, num_inference_steps, guidance_scale):\n",
    "        try:          \n",
    "            generation_kwargs = {\n",
    "                \"prompt\": prompt,\n",
    "                \"negative_prompt\": negative_prompt,\n",
    "                \"num_inference_steps\": num_inference_steps,\n",
    "                \"guidance_scale\": guidance_scale,\n",
    "                \"height\": 512,\n",
    "                \"width\": 512,\n",
    "            }\n",
    "            \n",
    "            # Generar imagen\n",
    "            with torch.autocast(\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "                result = self.pipe(**generation_kwargs)\n",
    "                image = result.images[0]\n",
    "            \n",
    "            # Convertir a base64\n",
    "            buffer = io.BytesIO()\n",
    "            image.save(buffer, format='PNG')\n",
    "            img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "            \n",
    "            # Guardar imagen\n",
    "            timestamp = int(time.time())\n",
    "            os.makedirs(\"images\", exist_ok=True)\n",
    "            image_filename = f\"images/stable_diffusion_{timestamp}.png\"\n",
    "            image.save(image_filename)\n",
    "            \n",
    "            print(f\"Imagen generada con Stable Diffusion: {image_filename}\")\n",
    "            return image, img_base64, image_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error con Stable Diffusion, usando placeholder: {e}\")\n",
    "            return self._generate_placeholder(prompt)\n",
    "    \n",
    "    def _generate_placeholder(self, prompt):\n",
    "        \"\"\"Genera una imagen placeholder visual\"\"\"\n",
    "        try:\n",
    "            timestamp = int(time.time())\n",
    "            hash_value = int(hashlib.md5(prompt.encode()).hexdigest()[:6], 16)\n",
    "            \n",
    "            r = (hash_value >> 16) & 255\n",
    "            g = (hash_value >> 8) & 255\n",
    "            b = hash_value & 255\n",
    "            \n",
    "            # Asegurar colores visibles\n",
    "            r = max(50, min(200, r))\n",
    "            g = max(50, min(200, g))\n",
    "            b = max(50, min(200, b))\n",
    "            \n",
    "            img = Image.new('RGB', (512, 512), color=(r, g, b))\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            \n",
    "            try:\n",
    "                words = prompt.split()[:4]\n",
    "                text = \" \".join(words)\n",
    "                \n",
    "                bbox = draw.textbbox((0, 0), text)\n",
    "                text_width = bbox[2] - bbox[0]\n",
    "                text_height = bbox[3] - bbox[1]\n",
    "                \n",
    "                x = (512 - text_width) // 2\n",
    "                y = (512 - text_height) // 2\n",
    "                \n",
    "                for dx in [-1, 0, 1]:\n",
    "                    for dy in [-1, 0, 1]:\n",
    "                        if dx != 0 or dy != 0:\n",
    "                            draw.text((x + dx, y + dy), text, fill=(0, 0, 0))\n",
    "                \n",
    "                draw.text((x, y), text, fill=(255, 255, 255))\n",
    "                \n",
    "                placeholder_text = \"PLACEHOLDER\"\n",
    "                bbox2 = draw.textbbox((0, 0), placeholder_text)\n",
    "                text_width2 = bbox2[2] - bbox2[0]\n",
    "                x2 = (512 - text_width2) // 2\n",
    "                y2 = 450\n",
    "                \n",
    "                draw.text((x2, y2), placeholder_text, fill=(200, 200, 200))\n",
    "                \n",
    "            except Exception as text_error:\n",
    "                print(f\"Error con texto: {text_error}\")\n",
    "                draw.ellipse([200, 200, 312, 312], fill=(255, 255, 255))\n",
    "            \n",
    "            buffer = io.BytesIO()\n",
    "            img.save(buffer, format='PNG')\n",
    "            img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "            \n",
    "            os.makedirs(\"images\", exist_ok=True)\n",
    "            image_filename = f\"images/placeholder_image_{timestamp}.png\"\n",
    "            img.save(image_filename)\n",
    "            \n",
    "            return img, img_base64, image_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generando imagen placeholder: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def chat_completions_create(self, messages):\n",
    "        \"\"\"Interfaz compatible con OpenAI para generación de imágenes\"\"\"\n",
    "        try:\n",
    "            user_message = \"\"\n",
    "            for msg in reversed(messages):\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    user_message = msg[\"content\"]\n",
    "                    break\n",
    "            \n",
    "            if not user_message:\n",
    "                raise Exception(\"No se encontró prompt para generar imagen\")\n",
    "            \n",
    "            image, img_base64, filename = self.generate_image(user_message)\n",
    "            \n",
    "            if self.use_placeholder or self.pipe == \"placeholder\":\n",
    "                response_content = f\"![Imagen generada](data:image/png;base64,{img_base64})\\n\"\n",
    "            else:\n",
    "                response_content = f\"![Imagen generada](data:image/png;base64,{img_base64})\\n\"\n",
    "            \n",
    "            class Choice:\n",
    "                def __init__(self, content):\n",
    "                    self.message = type('obj', (object,), {'content': content})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, content):\n",
    "                    self.choices = [Choice(content)]\n",
    "            \n",
    "            return Response(response_content)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"**Error generando imagen:** {str(e)}\"\n",
    "            print(error_msg)\n",
    "            \n",
    "            class Choice:\n",
    "                def __init__(self, error):\n",
    "                    self.message = type('obj', (object,), {'content': error})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, error):\n",
    "                    self.choices = [Choice(error)]\n",
    "            \n",
    "            return Response(error_msg)\n",
    "\n",
    "runwayml_model_instance = RunwaymlModel()\n",
    "\n",
    "runwayml_model = dict(\n",
    "    model=runwayml_model_instance, \n",
    "    model_name=\"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca0c5d",
   "metadata": {},
   "source": [
    "### stabilityai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f80257e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo gratuito de Hugging Face para generación de imágenes\n",
    "class HuggingFaceImageModel:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.model_name = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "        \n",
    "    def generate_image_hf(self, prompt):\n",
    "        try:\n",
    "            # Usar el cliente de Hugging Face para generar imagen\n",
    "            image = self.client.text_to_image(\n",
    "                prompt=prompt,\n",
    "                model=self.model_name\n",
    "            )\n",
    "            \n",
    "            # La imagen ya es un objeto PIL, no necesita conversión desde bytes\n",
    "            # Convertir a base64\n",
    "            buffer = io.BytesIO()\n",
    "            image.save(buffer, format='PNG')\n",
    "            img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "            \n",
    "            # Guardar imagen\n",
    "            timestamp = int(time.time())\n",
    "            os.makedirs(\"images\", exist_ok=True)\n",
    "            image_filename = f\"images/hf_sdxl_{timestamp}.png\"\n",
    "            image.save(image_filename)\n",
    "            \n",
    "            print(f\"Imagen generada con Hugging Face SDXL: {image_filename}\")\n",
    "            return image, img_base64, image_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error con Hugging Face, generando placeholder: {e}\")\n",
    "            return self._generate_placeholder(prompt)\n",
    "    \n",
    "    def _generate_placeholder(self, prompt):\n",
    "        \"\"\"Genera una imagen placeholder visual\"\"\"\n",
    "        try:\n",
    "            timestamp = int(time.time())\n",
    "            hash_value = int(hashlib.md5(prompt.encode()).hexdigest()[:6], 16)\n",
    "            \n",
    "            r = (hash_value >> 16) & 255\n",
    "            g = (hash_value >> 8) & 255\n",
    "            b = hash_value & 255\n",
    "            \n",
    "            r = max(50, min(200, r))\n",
    "            g = max(50, min(200, g))\n",
    "            b = max(50, min(200, b))\n",
    "            \n",
    "            img = Image.new('RGB', (512, 512), color=(r, g, b))\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            \n",
    "            try:\n",
    "                words = prompt.split()[:4]\n",
    "                text = \" \".join(words)\n",
    "                \n",
    "                bbox = draw.textbbox((0, 0), text)\n",
    "                text_width = bbox[2] - bbox[0]\n",
    "                text_height = bbox[3] - bbox[1]\n",
    "                \n",
    "                x = (512 - text_width) // 2\n",
    "                y = (512 - text_height) // 2\n",
    "                \n",
    "                for dx in [-1, 0, 1]:\n",
    "                    for dy in [-1, 0, 1]:\n",
    "                        if dx != 0 or dy != 0:\n",
    "                            draw.text((x + dx, y + dy), text, fill=(0, 0, 0))\n",
    "                \n",
    "                draw.text((x, y), text, fill=(255, 255, 255))\n",
    "                \n",
    "                placeholder_text = \"HF PLACEHOLDER\"\n",
    "                bbox2 = draw.textbbox((0, 0), placeholder_text)\n",
    "                text_width2 = bbox2[2] - bbox2[0]\n",
    "                x2 = (512 - text_width2) // 2\n",
    "                y2 = 450\n",
    "                \n",
    "                draw.text((x2, y2), placeholder_text, fill=(200, 200, 200))\n",
    "                \n",
    "            except Exception as text_error:\n",
    "                print(f\"Error con texto: {text_error}\")\n",
    "                draw.ellipse([200, 200, 312, 312], fill=(255, 255, 255))\n",
    "            \n",
    "            buffer = io.BytesIO()\n",
    "            img.save(buffer, format='PNG')\n",
    "            img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "            \n",
    "            os.makedirs(\"images\", exist_ok=True)\n",
    "            image_filename = f\"images/placeholder_hf_{timestamp}.png\"\n",
    "            img.save(image_filename)\n",
    "            \n",
    "            return img, img_base64, image_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generando placeholder: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def chat_completions_create(self, messages):\n",
    "        \"\"\"Interfaz compatible con OpenAI para generación de imágenes\"\"\"\n",
    "        try:\n",
    "            user_message = \"\"\n",
    "            for msg in reversed(messages):\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    user_message = msg[\"content\"]\n",
    "                    break\n",
    "            \n",
    "            if not user_message:\n",
    "                raise Exception(\"No se encontró prompt para generar imagen\")\n",
    "            \n",
    "            image, img_base64, filename = self.generate_image_hf(user_message)\n",
    "            \n",
    "            response_content = f\"![Imagen generada](data:image/png;base64,{img_base64})\\n**Generado con Stable Diffusion XL (Hugging Face)**\"\n",
    "            \n",
    "            class Choice:\n",
    "                def __init__(self, content):\n",
    "                    self.message = type('obj', (object,), {'content': content})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, content):\n",
    "                    self.choices = [Choice(content)]\n",
    "            \n",
    "            return Response(response_content)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"**Error generando imagen:** {str(e)}\"\n",
    "            print(error_msg)\n",
    "            \n",
    "            class Choice:\n",
    "                def __init__(self, error):\n",
    "                    self.message = type('obj', (object,), {'content': error})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, error):\n",
    "                    self.choices = [Choice(error)]\n",
    "            \n",
    "            return Response(error_msg)\n",
    "\n",
    "hf_image_model_instance = HuggingFaceImageModel(helsinki_client)\n",
    "\n",
    "stability_model = dict(\n",
    "    model=hf_image_model_instance,\n",
    "    model_name=\"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f01a9",
   "metadata": {},
   "source": [
    "## OpenRouterAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "670cfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_router_model = dict(model=OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=open_router_api_key,\n",
    "), \n",
    "model_name=\"deepseek/deepseek-r1-0528\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8d7ae",
   "metadata": {},
   "source": [
    "## Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e59a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_model = dict(model=OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\"), model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15574618",
   "metadata": {},
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f261382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ollama disponible con modelo: llama3.1:8b\n",
      "✅ Ollama disponible con modelo: mistral:7b\n",
      "✅ Ollama disponible con modelo: mistral:7b\n"
     ]
    }
   ],
   "source": [
    "class OllamaModel:\n",
    "    def __init__(self, model_name=\"llama3.1:8b\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = \"http://localhost:11434/v1\"\n",
    "        self.client = None\n",
    "        self.available = False\n",
    "        self._check_availability()\n",
    "    \n",
    "    def _check_availability(self):\n",
    "        \"\"\"Verifica si Ollama está disponible\"\"\"\n",
    "        try:\n",
    "            import requests\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                self.available = True\n",
    "                self.client = OpenAI(\n",
    "                    base_url=self.base_url,\n",
    "                    api_key=\"ollama\"\n",
    "                )\n",
    "                print(f\"✅ Ollama disponible con modelo: {self.model_name}\")\n",
    "            else:\n",
    "                self.available = False\n",
    "                print(\"⚠️ Ollama no está corriendo. Inicia Ollama con: ollama serve\")\n",
    "        except Exception as e:\n",
    "            self.available = False\n",
    "            print(f\"⚠️ Ollama no disponible: {e}\")\n",
    "    \n",
    "    def chat_completions_create(self, messages):\n",
    "        \"\"\"Interfaz compatible con OpenAI para Ollama\"\"\"\n",
    "        try:\n",
    "            if not self.available:\n",
    "                raise Exception(\"Ollama no está disponible. Asegúrate de que esté corriendo.\")\n",
    "            \n",
    "            resp = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=messages\n",
    "            )\n",
    "            return resp\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error con Ollama: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            \n",
    "            class Choice:\n",
    "                def __init__(self, error):\n",
    "                    self.message = type('obj', (object,), {'content': error})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, error):\n",
    "                    self.choices = [Choice(error)]\n",
    "            \n",
    "            return Response(error_msg)\n",
    "\n",
    "ollama_llama_instance = OllamaModel(\"llama3.1:8b\")\n",
    "ollama_mistral_instance = OllamaModel(\"mistral:7b\")\n",
    "\n",
    "ollama_llama_model = dict(\n",
    "    model=ollama_llama_instance,\n",
    "    model_name=\"llama3.1:8b\"\n",
    ")\n",
    "\n",
    "ollama_mistral_model = dict(\n",
    "    model=ollama_mistral_instance,\n",
    "    model_name=\"mistral:7b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33aea803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests disponible\n",
      "Ollama corriendo con 0 modelos\n",
      "Ollama corriendo con 0 modelos\n"
     ]
    }
   ],
   "source": [
    "# Verificar estado de Ollama\n",
    "try:\n",
    "    print(\"requests disponible\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get(\"models\", [])\n",
    "            print(f\"Ollama corriendo con {len(models)} modelos\")\n",
    "            for model in models[:3]:\n",
    "                print(f\"   - {model.get('name', 'Unknown')}\")\n",
    "        else:\n",
    "            print(\"Ollama no responde\")\n",
    "    except:\n",
    "        print(\"Ollama no está corriendo (ver README.md para instalación)\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Instala requests: pip install requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c15b1a",
   "metadata": {},
   "source": [
    "## Funcion de chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f1a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_data = {\n",
    "    \"total_requests\": 0,\n",
    "    \"successful_requests\": 0,\n",
    "    \"failed_requests\": 0,\n",
    "    \"requests_by_model\": {},\n",
    "    \"requests_by_task\": {},\n",
    "    \"response_times\": [],\n",
    "    \"errors\": [],\n",
    "    \"session_start\": datetime.now().isoformat(),\n",
    "    \"last_request\": None,\n",
    "    \"images_generated\": 0,\n",
    "    \"translations_made\": 0\n",
    "}\n",
    "\n",
    "def log_metric(metric_type, **kwargs):\n",
    "    \"\"\"Registra métricas del sistema\"\"\"\n",
    "    global metrics_data\n",
    "    \n",
    "    try:\n",
    "        current_time = datetime.now().isoformat()\n",
    "        \n",
    "        if metric_type == \"request_start\":\n",
    "            metrics_data[\"total_requests\"] += 1\n",
    "            metrics_data[\"last_request\"] = current_time\n",
    "            \n",
    "            # Contar por modelo\n",
    "            model = kwargs.get(\"model\", \"unknown\")\n",
    "            if model not in metrics_data[\"requests_by_model\"]:\n",
    "                metrics_data[\"requests_by_model\"][model] = 0\n",
    "            metrics_data[\"requests_by_model\"][model] += 1\n",
    "            \n",
    "            # Contar por tarea\n",
    "            task = kwargs.get(\"task\", \"unknown\")\n",
    "            if task not in metrics_data[\"requests_by_task\"]:\n",
    "                metrics_data[\"requests_by_task\"][task] = 0\n",
    "            metrics_data[\"requests_by_task\"][task] += 1\n",
    "            \n",
    "        elif metric_type == \"request_success\":\n",
    "            metrics_data[\"successful_requests\"] += 1\n",
    "            response_time = kwargs.get(\"response_time\", 0)\n",
    "            metrics_data[\"response_times\"].append(response_time)\n",
    "            \n",
    "            # Contar tipos específicos\n",
    "            task = kwargs.get(\"task\", \"\")\n",
    "            if task == \"imagenes\":\n",
    "                metrics_data[\"images_generated\"] += 1\n",
    "            elif task == \"traduccion\":\n",
    "                metrics_data[\"translations_made\"] += 1\n",
    "                \n",
    "        elif metric_type == \"request_error\":\n",
    "            metrics_data[\"failed_requests\"] += 1\n",
    "            error_info = {\n",
    "                \"timestamp\": current_time,\n",
    "                \"model\": kwargs.get(\"model\", \"unknown\"),\n",
    "                \"task\": kwargs.get(\"task\", \"unknown\"),\n",
    "                \"error\": kwargs.get(\"error\", \"Unknown error\"),\n",
    "                \"user_message\": kwargs.get(\"user_message\", \"\")[:100]  # Primeros 100 chars\n",
    "            }\n",
    "            metrics_data[\"errors\"].append(error_info)\n",
    "            \n",
    "            # Mantener solo los últimos 50 errores para no usar demasiada memoria\n",
    "            if len(metrics_data[\"errors\"]) > 50:\n",
    "                metrics_data[\"errors\"] = metrics_data[\"errors\"][-50:]\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error registrando métrica: {e}\")\n",
    "\n",
    "def get_metrics_summary():\n",
    "    try:\n",
    "        response_times = metrics_data[\"response_times\"]\n",
    "        if response_times:\n",
    "            avg_response_time = sum(response_times) / len(response_times)\n",
    "            min_response_time = min(response_times)\n",
    "            max_response_time = max(response_times)\n",
    "        else:\n",
    "            avg_response_time = min_response_time = max_response_time = 0\n",
    "        \n",
    "        # Calcular tasa de éxito\n",
    "        total = metrics_data[\"total_requests\"]\n",
    "        success_rate = (metrics_data[\"successful_requests\"] / total * 100) if total > 0 else 0\n",
    "        \n",
    "        # Tiempo de sesión\n",
    "        session_start = datetime.fromisoformat(metrics_data[\"session_start\"])\n",
    "        session_duration = datetime.now() - session_start\n",
    "        \n",
    "        # Modelo más usado\n",
    "        model_usage = metrics_data[\"requests_by_model\"]\n",
    "        most_used_model = max(model_usage.items(), key=lambda x: x[1]) if model_usage else (\"Ninguno\", 0)\n",
    "        \n",
    "        # Tarea más usada\n",
    "        task_usage = metrics_data[\"requests_by_task\"]\n",
    "        most_used_task = max(task_usage.items(), key=lambda x: x[1]) if task_usage else (\"Ninguna\", 0)\n",
    "        \n",
    "        summary = f\"\"\"# **Métricas del ChatBot**\n",
    "\n",
    "## **Estadísticas Generales**\n",
    "- **Total de solicitudes:** {total}\n",
    "- **Solicitudes exitosas:** {metrics_data[\"successful_requests\"]} ({success_rate:.1f}%)\n",
    "- **Solicitudes fallidas:** {metrics_data[\"failed_requests\"]}\n",
    "- **Imágenes generadas:** {metrics_data[\"images_generated\"]}\n",
    "- **Traducciones realizadas:** {metrics_data[\"translations_made\"]}\n",
    "\n",
    "## **Tiempos de Respuesta**\n",
    "- **Promedio:** {avg_response_time:.2f}s\n",
    "- **Mínimo:** {min_response_time:.2f}s\n",
    "- **Máximo:** {max_response_time:.2f}s\n",
    "- **Total de mediciones:** {len(response_times)}\n",
    "\n",
    "## **Uso por Modelo**\n",
    "\"\"\"\n",
    "        # Agregar uso por modelo\n",
    "        for model, count in sorted(model_usage.items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / total * 100) if total > 0 else 0\n",
    "            summary += f\"- **{model.capitalize()}:** {count} ({percentage:.1f}%)\\n\"\n",
    "        \n",
    "        summary += f\"\\n## **Uso por Tarea**\\n\"\n",
    "        \n",
    "        # Agregar uso por tarea\n",
    "        for task, count in sorted(task_usage.items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / total * 100) if total > 0 else 0\n",
    "            summary += f\"- **{task.capitalize()}:** {count} ({percentage:.1f}%)\\n\"\n",
    "        \n",
    "        summary += f\"\"\"\n",
    "## **Información de Sesión**\n",
    "- **Inicio de sesión:** {session_start.strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "- **Duración:** {str(session_duration).split('.')[0]}\n",
    "- **Última solicitud:** {metrics_data[\"last_request\"] or \"Ninguna\"}\n",
    "\n",
    "## **Más Utilizados**\n",
    "- **Modelo más usado:** {most_used_model[0]} ({most_used_model[1]} usos)\n",
    "- **Tarea más usada:** {most_used_task[0]} ({most_used_task[1]} usos)\n",
    "\"\"\"\n",
    "        # Agregar errores recientes si los hay\n",
    "        recent_errors = metrics_data[\"errors\"][-5:] if metrics_data[\"errors\"] else []\n",
    "        if recent_errors:\n",
    "            summary += \"\\n## **Errores Recientes (Últimos 5)**\\n\"\n",
    "            for i, error in enumerate(recent_errors, 1):\n",
    "                timestamp = error[\"timestamp\"][:19].replace(\"T\", \" \")  # Formato legible\n",
    "                summary += f\"{i}. **[{timestamp}]** {error['model']} - {error['task']}: {error['error'][:100]}...\\n\"\n",
    "        \n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"**Error generando métricas:** {str(e)}\"\n",
    "\n",
    "def reset_metrics():\n",
    "    global metrics_data\n",
    "    metrics_data = {\n",
    "        \"total_requests\": 0,\n",
    "        \"successful_requests\": 0,\n",
    "        \"failed_requests\": 0,\n",
    "        \"requests_by_model\": {},\n",
    "        \"requests_by_task\": {},\n",
    "        \"response_times\": [],\n",
    "        \"errors\": [],\n",
    "        \"session_start\": datetime.now().isoformat(),\n",
    "        \"last_request\": None,\n",
    "        \"images_generated\": 0,\n",
    "        \"translations_made\": 0\n",
    "    }\n",
    "    return \"**Métricas reiniciadas correctamente**\"\n",
    "\n",
    "def obtener_valor(dict: dict, key: str):\n",
    "    return dict.get(key, \"Clave no encontrada\")\n",
    "\n",
    "models = dict(\n",
    "    gemini=gemini_model,\n",
    "    open_router=open_router_model,\n",
    "    groq=groq_model,\n",
    "    helsinki=helsinki_model,  \n",
    "    runwayml=runwayml_model,\n",
    "    stabilityai=stability_model,\n",
    "    ollama_llama=ollama_llama_model,\n",
    "    ollama_mistral=ollama_mistral_model\n",
    ")\n",
    "\n",
    "providers_config = {\n",
    "    \"openai\": {\n",
    "        \"models\": [\"gemini\", \"groq\", \"open_router\"],\n",
    "        \"description\": \"Modelos basados en APIs comerciales (OpenAI-compatible)\"\n",
    "    },\n",
    "    \"huggingface\": {\n",
    "        \"models\": [\"helsinki\", \"stabilityai\"],\n",
    "        \"description\": \"Modelos gratuitos de Hugging Face\"\n",
    "    },\n",
    "    \"local\": {\n",
    "        \"models\": [\"runwayml\"],\n",
    "        \"description\": \"Modelos ejecutados localmente\"\n",
    "    },\n",
    "    \"ollama\": {\n",
    "        \"models\": [\"ollama_llama\", \"ollama_mistral\"],\n",
    "        \"description\": \"Modelos locales ejecutados con Ollama\"\n",
    "    },\n",
    "    \"todos\": {\n",
    "        \"models\": [\"gemini\", \"groq\", \"open_router\", \"helsinki\", \"stabilityai\", \"runwayml\", \"ollama_llama\", \"ollama_mistral\"],\n",
    "        \"description\": \"Todos los modelos disponibles\"\n",
    "    }\n",
    "}\n",
    "\n",
    "tasks_config = {\n",
    "    \"traduccion\": {\n",
    "        \"models\": [\"gemini\", \"helsinki\", \"ollama_llama\"], \n",
    "        \"description\": \"Traducir texto entre idiomas (Helsinki: solo inglés → español)\"\n",
    "    },\n",
    "    \"resumen\": {\n",
    "        \"models\": [\"groq\", \"open_router\", \"ollama_mistral\"],\n",
    "        \"description\": \"Resumir texto o documentos (Ollama: procesamiento local)\"\n",
    "    },\n",
    "    \"imagenes\": {\n",
    "        \"models\": [\"runwayml\", \"stabilityai\"],  \n",
    "        \"description\": \"Generar imágenes usando Stable Diffusion local o Hugging Face SDXL\"\n",
    "    }\n",
    "}\n",
    "sent_files = []\n",
    "\n",
    "def get_available_models(task):\n",
    "    \"\"\"Obtiene los modelos disponibles para una tarea específica\"\"\"\n",
    "    return tasks_config.get(task, {}).get(\"models\", [])\n",
    "\n",
    "def get_available_models_by_provider(provider):\n",
    "    \"\"\"Obtiene los modelos disponibles para un proveedor específico\"\"\"\n",
    "    return providers_config.get(provider, {}).get(\"models\", [])\n",
    "\n",
    "def validate_input(message):\n",
    "    \"\"\"Valida el mensaje de entrada\"\"\"\n",
    "    if not message:\n",
    "        return False, \"**Error**: No puedes enviar un mensaje vacío. Por favor, escribe algo.\"\n",
    "    \n",
    "    if len(message.strip()) == 0:\n",
    "        return False, \"**Error**: El mensaje solo contiene espacios en blanco. Por favor, escribe un mensaje válido.\"\n",
    "    \n",
    "    if len(message) > 8000:\n",
    "        return False, \"**Error**: El mensaje es demasiado largo (máximo 8000 caracteres). Por favor, acórtalo.\"\n",
    "    \n",
    "    return True, None\n",
    "\n",
    "def format_inference_time(seconds):\n",
    "    if seconds < 1:\n",
    "        return f\"{seconds*1000:.0f}ms\"\n",
    "    elif seconds < 60:\n",
    "        return f\"{seconds:.2f}s\"\n",
    "    else:\n",
    "        minutes = int(seconds // 60)\n",
    "        remaining_seconds = seconds % 60\n",
    "        return f\"{minutes}m {remaining_seconds:.1f}s\"\n",
    "\n",
    "def create_initial_chat():\n",
    "    chat_id = f\"Chat-{str(uuid.uuid4())[:8]}\"\n",
    "    return chat_id, []\n",
    "\n",
    "def chat(message: dict, history, model, task, provider=None):   \n",
    "    total_start_time = time.time()\n",
    "    log_metric(\"request_start\", model=model, task=task, user_message=message)\n",
    "    is_valid, error_msg = validate_input(message[\"text\"])\n",
    "    if not is_valid:\n",
    "        log_metric(\"request_error\", model=model, task=task, error=\"Invalid input\", user_message=message)\n",
    "        return error_msg\n",
    "    try:\n",
    "        print(f\"Tarea seleccionada: {task}\")\n",
    "        print(f\"Modelo seleccionado: {model}\")\n",
    "        print(f\"Proveedor seleccionado: {provider}\")\n",
    "        \n",
    "        # Validar por proveedor si se especifica\n",
    "        if provider:\n",
    "            if provider not in providers_config:\n",
    "                error = f\"Proveedor '{provider}' no reconocido\"\n",
    "                log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                return f\"❌ **Error**: {error}. Proveedores disponibles: {', '.join(providers_config.keys())}\"\n",
    "            \n",
    "            available_models_provider = get_available_models_by_provider(provider)\n",
    "            if not available_models_provider:\n",
    "                error = f\"No hay modelos disponibles para el proveedor '{provider}'\"\n",
    "                log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                return f\"❌ **Error**: {error}.\"\n",
    "            \n",
    "            if model not in available_models_provider:\n",
    "                error = f\"El modelo '{model}' no está disponible para el proveedor '{provider}'\"\n",
    "                log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                return f\"❌ **Error**: {error}. Modelos disponibles: {', '.join(available_models_provider)}\"\n",
    "        \n",
    "        # Validar por tarea\n",
    "        if task not in tasks_config:\n",
    "            error = f\"Tarea '{task}' no reconocida\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"❌ **Error**: {error}. Tareas disponibles: {', '.join(tasks_config.keys())}\"\n",
    "        \n",
    "        available_models = get_available_models(task)\n",
    "        if not available_models:\n",
    "            error = f\"No hay modelos disponibles para la tarea '{task}'\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"❌ **Error**: {error}.\"\n",
    "        \n",
    "        if model not in available_models:\n",
    "            error = f\"El modelo '{model}' no está disponible para la tarea '{task}'\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"❌ **Error**: {error}. Modelos disponibles: {', '.join(available_models)}\"\n",
    "        \n",
    "        if model not in models:\n",
    "            error = f\"El modelo '{model}' no está configurado en el sistema\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"❌ **Error**: {error}.\"\n",
    "        \n",
    "        model_config = models[model]\n",
    "        if not model_config or obtener_valor(model_config, \"model\") == \"Clave no encontrada\":\n",
    "            error = f\"El modelo '{model}' no está disponible. Verifica la configuración de API keys\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"❌ **Error**: {error}.\"\n",
    "\n",
    "        print(f\"modelo: {obtener_valor(model_config, 'model_name')}\")\n",
    "\n",
    "        system_prompt = \"Eres un asistente de IA que responde preguntas y ayuda con tareas.\"\n",
    "        \n",
    "        if task == \"traduccion\":\n",
    "            if model == \"helsinki\":\n",
    "                system_prompt = \"Eres un traductor especializado que traduce ÚNICAMENTE de inglés a español usando modelos Helsinki-NLP. Solo acepta texto en inglés y lo traduce al español.\"\n",
    "            elif model.startswith(\"ollama\"):\n",
    "                system_prompt = \"Eres un traductor experto que puede traducir entre múltiples idiomas de manera precisa y natural. Identifica automáticamente el idioma de origen y traduce al idioma solicitado.\"\n",
    "            else:\n",
    "                system_prompt = \"Eres un traductor experto. Tu tarea es traducir texto entre diferentes idiomas de manera precisa y natural.\"\n",
    "        elif task == \"resumen\":\n",
    "            if model.startswith(\"ollama\"):\n",
    "                system_prompt = \"Eres un asistente especializado en resumir texto de forma clara y concisa. Procesas el contenido localmente para mantener la privacidad de los datos.\"\n",
    "            else:\n",
    "                system_prompt = \"Eres un asistente especializado en resumir texto. Tu tarea es crear resúmenes claros y concisos del contenido proporcionado.\"\n",
    "        elif task == \"imagenes\":\n",
    "            if model == \"runwayml\":\n",
    "                system_prompt = \"Eres un generador de imágenes que usa Stable Diffusion cuando está disponible, o genera placeholders visuales cuando hay problemas técnicos. Conviertes descripciones de texto en imágenes.\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"{system_prompt}\"}\n",
    "        ]\n",
    "\n",
    "        if message[\"files\"]:\n",
    "            sent_files.append(message[\"files\"][0])\n",
    "            for file in message[\"files\"]:\n",
    "                if os.path.basename(file).endswith('.pdf'):\n",
    "                    file_content = read_file_pdf(file)\n",
    "                elif os.path.basename(file).endswith('.txt'):\n",
    "                    file_content = read_file_txt(file)\n",
    "                elif os.path.basename(file).endswith('.docx'):\n",
    "                    file_content = read_file_docx(file)\n",
    "                else:\n",
    "                    file_content = \"Tipo de archivo no soportado. Solo se permiten archivos .txt, .pdf y .docx\"\n",
    "                \n",
    "                # Añadir el contenido del archivo al historial como un mensaje del sistema\n",
    "                history.append({\"role\": \"system\", \"content\": f\"Contenido del archivo \\\"{os.path.basename(file)}\\\": {file_content}\"})\n",
    "\n",
    "        for msg in history:\n",
    "            if valid_history(msg, sent_files[0] if sent_files else \"\"):\n",
    "                messages.append({\"role\": msg[\"role\"], \"content\": msg[\"content\"]})\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": message[\"text\"]})\n",
    "        model_instance = obtener_valor(model_config, \"model\")\n",
    "        response_content = None\n",
    "        \n",
    "        inference_start_time = None\n",
    "        inference_end_time = None\n",
    "        total_attempts = 0\n",
    "        \n",
    "        max_retries = 2\n",
    "        for attempt in range(max_retries + 1):\n",
    "            try:\n",
    "                total_attempts += 1\n",
    "                \n",
    "                inference_start_time = time.time()\n",
    "                if model in [\"helsinki\", \"runwayml\", \"stabilityai\"]:\n",
    "                    resp = model_instance.chat_completions_create(\n",
    "                        messages=messages\n",
    "                    )\n",
    "                elif model.startswith(\"ollama\"):\n",
    "                    resp = model_instance.chat_completions_create(\n",
    "                        messages=messages\n",
    "                    )\n",
    "                else:\n",
    "                    resp = model_instance.chat.completions.create(\n",
    "                        model=obtener_valor(model_config, \"model_name\"),\n",
    "                        messages=messages\n",
    "                    )\n",
    "                    \n",
    "                inference_end_time = time.time()\n",
    "\n",
    "                if not resp or not hasattr(resp, 'choices') or len(resp.choices) == 0:\n",
    "                    raise Exception(\"Respuesta vacía del modelo\")\n",
    "                \n",
    "                if not hasattr(resp.choices[0], 'message') or not hasattr(resp.choices[0].message, 'content'):\n",
    "                    raise Exception(\"Formato de respuesta inválido\")\n",
    "                \n",
    "                response_content = resp.choices[0].message.content\n",
    "                \n",
    "                if not response_content or len(response_content.strip()) == 0:\n",
    "                    raise Exception(\"El modelo devolvió una respuesta vacía\")\n",
    "                \n",
    "                break \n",
    "                \n",
    "            except Timeout:\n",
    "                inference_end_time = time.time()  \n",
    "                if attempt < max_retries:\n",
    "                    print(f\"Timeout en intento {attempt + 1}, reintentando...\")\n",
    "                    time.sleep(2)  \n",
    "                    continue\n",
    "                else:\n",
    "                    error = f\"El modelo '{model}' tardó demasiado en responder\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"⏱**Error de Timeout**: {error}. Intenta de nuevo en unos momentos.\"\n",
    "            \n",
    "            except ConnectionError:\n",
    "                inference_end_time = time.time()  \n",
    "                if attempt < max_retries:\n",
    "                    print(f\"Error de conexión en intento {attempt + 1}, reintentando...\")\n",
    "                    time.sleep(2)\n",
    "                    continue\n",
    "                else:\n",
    "                    error = f\"No se pudo conectar con el modelo '{model}'\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"**Error de Conexión**: {error}. Verifica tu conexión a internet.\"\n",
    "            \n",
    "            except Exception as api_error:\n",
    "                inference_end_time = time.time()\n",
    "                error_msg = str(api_error).lower()\n",
    "                \n",
    "                if \"rate limit\" in error_msg or \"quota\" in error_msg:\n",
    "                    error = f\"Límite de uso alcanzado para el modelo '{model}'\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"**Límite de Uso**: {error}. Intenta más tarde o cambia de modelo.\"\n",
    "                \n",
    "                elif \"authentication\" in error_msg or \"unauthorized\" in error_msg or \"api key\" in error_msg:\n",
    "                    error = f\"Error de autenticación del modelo '{model}'\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"**Error de Autenticación**: La API key del modelo '{model}' es inválida o ha expirado. Verifica tu configuración.\"\n",
    "                \n",
    "                elif \"not found\" in error_msg or \"404\" in error_msg:\n",
    "                    error = f\"El modelo '{obtener_valor(model_config, 'model_name')}' no está disponible\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"**Modelo No Encontrado**: {error} en este momento.\"\n",
    "                \n",
    "                elif \"server error\" in error_msg or \"500\" in error_msg or \"502\" in error_msg or \"503\" in error_msg:\n",
    "                    if attempt < max_retries:\n",
    "                        print(f\"Error del servidor en intento {attempt + 1}, reintentando...\")\n",
    "                        time.sleep(3)\n",
    "                        continue\n",
    "                    else:\n",
    "                        error = f\"El servicio del modelo '{model}' está temporalmente no disponible\"\n",
    "                        log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                        return f\"🔧 **Error del Servidor**: {error}. Intenta más tarde.\"\n",
    "                \n",
    "                elif \"content policy\" in error_msg or \"safety\" in error_msg:\n",
    "                    error = f\"Contenido bloqueado por las políticas de seguridad del modelo '{model}'\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"⚠️**Contenido Bloqueado**: {error}. Reformula tu pregunta.\"\n",
    "                \n",
    "                else:\n",
    "                    if attempt < max_retries:\n",
    "                        print(f\"Error genérico en intento {attempt + 1}: {api_error}\")\n",
    "                        time.sleep(2)\n",
    "                        continue\n",
    "                    else:\n",
    "                        error = f\"Problema con el modelo '{model}': {str(api_error)[:100]}\"\n",
    "                        log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                        return f\"**Error**: {error}...\"\n",
    "\n",
    "        if not response_content:\n",
    "            error = f\"No se pudo obtener una respuesta válida del modelo '{model}' después de {max_retries + 1} intentos\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"**Error**: {error}.\"\n",
    "        \n",
    "        total_end_time = time.time()\n",
    "        total_time = total_end_time - total_start_time\n",
    "        \n",
    "        if inference_start_time and inference_end_time:\n",
    "            inference_time = inference_end_time - inference_start_time\n",
    "        else:\n",
    "            inference_time = total_time\n",
    "        \n",
    "        log_metric(\"request_success\", model=model, task=task, response_time=inference_time)\n",
    "        \n",
    "        model_name = obtener_valor(model_config, \"model_name\")\n",
    "        model_display = f\"{model.capitalize()}\"\n",
    "        if model_name and model_name != \"Clave no encontrada\":\n",
    "            model_display += f\" ({model_name})\"\n",
    "        \n",
    "        time_metadata = f\"<small style='color: #888; font-size: 0.85em;'>⚡ {format_inference_time(inference_time)}\"\n",
    "        \n",
    "        if total_attempts > 1:\n",
    "            time_metadata += f\" ({total_attempts}º intento)\"\n",
    "        \n",
    "        time_metadata += f\" • {model_display}</small>\"\n",
    "        \n",
    "        final_response = response_content + \"\\n\\n\" + time_metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        total_end_time = time.time()\n",
    "        total_time = total_end_time - total_start_time\n",
    "        error = f\"Error inesperado: {str(e)[:200]}\"\n",
    "        log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "        print(f\"Error inesperado: {e}\")\n",
    "        return f\"❌ **Error Inesperado**: Ocurrió un problema técnico: {str(e)[:200]}... \\n\\n<small style='color: #888;'>⏱️ {format_inference_time(total_time)}</small>\"\n",
    "        \n",
    "    return final_response    \n",
    "    \n",
    "def valid_history(history, file):\n",
    "    if len(history[\"content\"]) > 0 and isinstance(history[\"content\"], tuple):\n",
    "        if history[\"content\"][0] == file:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "def read_file_txt(file):\n",
    "    try:\n",
    "        if file is None:\n",
    "            return \"\"\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"Error al leer el archivo: {e}\"\n",
    "    \n",
    "def read_file_pdf(file):\n",
    "    try:\n",
    "        with fitz.open(file) as doc:\n",
    "            text = \"\"\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error al leer el archivo PDF: {e}\"\n",
    "    \n",
    "def read_file_docx(file):\n",
    "    try:\n",
    "        texto = docx2txt.process(file)\n",
    "        return texto\n",
    "    except Exception as e:\n",
    "        return f\"Error al leer DOCX {file}: {e}\"\n",
    "    \n",
    "def update_models_for_task(task):\n",
    "    try:\n",
    "        available_models = get_available_models(task)\n",
    "        if available_models:\n",
    "            return gr.Dropdown(choices=available_models, value=available_models[0])\n",
    "        else:\n",
    "            return gr.Dropdown(choices=[], value=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al actualizar modelos: {e}\")\n",
    "        return gr.Dropdown(choices=[], value=None)\n",
    "\n",
    "def update_models_for_provider(provider):\n",
    "    try:\n",
    "        available_models = get_available_models_by_provider(provider)\n",
    "        if available_models:\n",
    "            return gr.Dropdown(choices=available_models, value=available_models[0])\n",
    "        else:\n",
    "            return gr.Dropdown(choices=[], value=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al actualizar modelos por proveedor: {e}\")\n",
    "        return gr.Dropdown(choices=[], value=None)\n",
    "\n",
    "def update_models_for_task_and_provider(task, provider):\n",
    "    try:\n",
    "        # Si el proveedor es \"todos\", usar los modelos de la tarea\n",
    "        if provider == \"todos\":\n",
    "            available_models = get_available_models(task)\n",
    "        else:\n",
    "            # Obtener modelos del proveedor que también estén disponibles para la tarea\n",
    "            provider_models = get_available_models_by_provider(provider)\n",
    "            task_models = get_available_models(task)\n",
    "            available_models = [model for model in provider_models if model in task_models]\n",
    "        \n",
    "        if available_models:\n",
    "            return gr.Dropdown(choices=available_models, value=available_models[0])\n",
    "        else:\n",
    "            return gr.Dropdown(choices=[], value=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al actualizar modelos por tarea y proveedor: {e}\")\n",
    "        return gr.Dropdown(choices=[], value=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206937d",
   "metadata": {},
   "source": [
    "## Creacion de la interfaz del Chat con Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0853a952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as app:\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## Configuración del Chat\")\n",
    "            \n",
    "            provider_selector = gr.Dropdown(\n",
    "                choices=list(providers_config.keys()),\n",
    "                value=\"todos\",\n",
    "                label=\"Proveedor\"\n",
    "            )\n",
    "            \n",
    "            task_selector = gr.Dropdown(\n",
    "                choices=list(tasks_config.keys()),\n",
    "                value=\"traduccion\",\n",
    "                label=\"Tipo de tarea\"\n",
    "            )\n",
    "            \n",
    "            model_selector = gr.Dropdown(\n",
    "                choices=get_available_models(\"traduccion\"),\n",
    "                value=\"gemini\" if get_available_models(\"traduccion\") else None,\n",
    "                label=\"Modelo de IA\"\n",
    "            )\n",
    "            \n",
    "            provider_info = gr.Markdown(f\"**Proveedor:** {providers_config['todos']['description']}\")\n",
    "            task_info = gr.Markdown(f\"**Tarea:** {tasks_config['traduccion']['description']}\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                metrics_btn = gr.Button(\"📊 Ver Métricas\", size=\"sm\", variant=\"secondary\")\n",
    "                reset_metrics_btn = gr.Button(\"🔄 Reiniciar Métricas\", size=\"sm\", variant=\"secondary\")\n",
    "            \n",
    "            metrics_display = gr.Markdown(\n",
    "                value=\"Haz clic en **Ver Métricas** para mostrar las estadísticas de uso.\",\n",
    "                visible=False,\n",
    "                label=\"Métricas del Sistema\"\n",
    "            )\n",
    "            \n",
    "            metrics_visible = gr.State(False)\n",
    "\n",
    "        with gr.Column(scale=3):\n",
    "            current_model = gr.State(\"gemini\")\n",
    "            current_task = gr.State(\"traduccion\")\n",
    "            current_provider = gr.State(\"todos\")\n",
    "\n",
    "            # Actualizar cuando cambie el proveedor\n",
    "            provider_selector.change(\n",
    "                fn=lambda provider, task: [\n",
    "                    update_models_for_task_and_provider(task, provider),\n",
    "                    f\"**Proveedor:** {providers_config.get(provider, {}).get('description', '')}\",\n",
    "                    provider,\n",
    "                    get_available_models_by_provider(provider)[0] if provider != \"todos\" and get_available_models_by_provider(provider) else (get_available_models(task)[0] if get_available_models(task) else None)\n",
    "                ],\n",
    "                inputs=[provider_selector, current_task],\n",
    "                outputs=[model_selector, provider_info, current_provider, current_model]\n",
    "            )\n",
    "\n",
    "            # Actualizar cuando cambie la tarea\n",
    "            task_selector.change(\n",
    "                fn=lambda task, provider: [\n",
    "                    update_models_for_task_and_provider(task, provider),\n",
    "                    f\"**Tarea:** {tasks_config.get(task, {}).get('description', '')}\", \n",
    "                    task,\n",
    "                    get_available_models(task)[0] if get_available_models(task) else None\n",
    "                ],\n",
    "                inputs=[task_selector, current_provider],\n",
    "                outputs=[model_selector, task_info, current_task, current_model]\n",
    "            )\n",
    "\n",
    "            # Actualizar cuando cambie el modelo\n",
    "            model_selector.change(\n",
    "                fn=lambda model: model,\n",
    "                inputs=model_selector,\n",
    "                outputs=current_model\n",
    "            )\n",
    "\n",
    "            chatbot = gr.Chatbot(type=\"messages\", height=500)\n",
    "\n",
    "            chat_interface = gr.ChatInterface(chat, \n",
    "                             chatbot=chatbot, \n",
    "                             additional_inputs=[current_model, current_task, current_provider],\n",
    "                             type=\"messages\", \n",
    "                             title=\"Chat con Modelos de IA\", \n",
    "                             multimodal=True,\n",
    "                             save_history=True,\n",
    "                             autoscroll=True,\n",
    "                             stop_btn=True\n",
    "                            )\n",
    "            # Función para toggle de métricas\n",
    "            def toggle_metrics(is_visible):\n",
    "                if is_visible:\n",
    "                    return False, gr.Markdown(visible=False)\n",
    "                else:\n",
    "                    metrics_content = get_metrics_summary()\n",
    "                    return True, gr.Markdown(value=metrics_content, visible=True)\n",
    "\n",
    "            # Función para reiniciar métricas\n",
    "            def reset_metrics_display(is_visible):\n",
    "                result = reset_metrics()\n",
    "                if is_visible:\n",
    "                    return is_visible, gr.Markdown(value=result, visible=True)\n",
    "                else:\n",
    "                    return is_visible, gr.Markdown(visible=False)\n",
    "\n",
    "            metrics_btn.click(\n",
    "                fn=toggle_metrics,\n",
    "                inputs=metrics_visible,\n",
    "                outputs=[metrics_visible, metrics_display]\n",
    "            )\n",
    "\n",
    "            reset_metrics_btn.click(\n",
    "                fn=reset_metrics_display,\n",
    "                inputs=metrics_visible,\n",
    "                outputs=[metrics_visible, metrics_display]\n",
    "            )\n",
    "\n",
    "app.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

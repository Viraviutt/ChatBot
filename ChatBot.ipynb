{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b732e7e",
   "metadata": {},
   "source": [
    "# Instalar dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b3f9063f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' %pip install openai python-dotenv gradio \\n%pip install --upgrade diffusers transformers accelerate scipy safetensors\\n%pip install --upgrade huggingface_hub[hf_xet]  # Incluye hf_xet para mejor rendimiento\\n%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # Para GPU CUDA 12.1\\n%pip install Pillow  # Para manipulación de imágenes\\n%pip install deep-translator  # Para traducir prompts automáticamente '"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instalar dependencias - ejecutar si es la primera vez\n",
    "\"\"\" %pip install openai python-dotenv gradio \n",
    "%pip install --upgrade diffusers transformers accelerate scipy safetensors\n",
    "%pip install --upgrade huggingface_hub[hf_xet]  # Incluye hf_xet para mejor rendimiento\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # Para GPU CUDA 12.1\n",
    "%pip install Pillow  # Para manipulación de imágenes\n",
    "%pip install deep-translator  # Para traducir prompts automáticamente \"\"\"\n",
    "\n",
    "# Si hay errores de compatibilidad, intenta:\n",
    "#pip install diffusers transformers --force-reinstall\n",
    "#pip install diffusers==0.21.4 transformers==4.35.2 --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2d56ed",
   "metadata": {},
   "source": [
    "# Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3e63fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from PIL import Image, ImageDraw\n",
    "import requests\n",
    "from requests.exceptions import Timeout, ConnectionError, RequestException\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import InferenceClient\n",
    "import torch \n",
    "from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fc9fce",
   "metadata": {},
   "source": [
    "# Definir las variables de las API KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6e549807",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "open_router_api_key = os.getenv('OPEN_ROUTER_API_KEY')\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "hf_token = os.getenv('HF_TOKEN')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2349589",
   "metadata": {},
   "source": [
    "### Modelos no disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dd0cc46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN configurado correctamente.\n"
     ]
    }
   ],
   "source": [
    "if not hf_token:\n",
    "    print(\"⚠️Advertencia: HF_TOKEN no encontrado. Helsinki no funcionará.\")\n",
    "else:\n",
    "    print(\"HF_TOKEN configurado correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7008d4fa",
   "metadata": {},
   "source": [
    "# Conectar los distintos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5934f57d",
   "metadata": {},
   "source": [
    "## Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6a1bf513",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model = dict(model=OpenAI(api_key=gemini_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"), model_name=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f54819",
   "metadata": {},
   "source": [
    "## Helsinki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0f9e11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "helsinki_client = InferenceClient(\n",
    "    api_key=os.getenv(\"HF_TOKEN\"), \n",
    ")\n",
    "\n",
    "class HelsinkiModel:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "        \n",
    "    def translate(self, text):\n",
    "        try:\n",
    "            result = self.client.translation(text, model=self.model_name)\n",
    "            \n",
    "            if isinstance(result, list) and len(result) > 0:\n",
    "                return result[0].get(\"translation_text\", result[0])\n",
    "            elif isinstance(result, dict):\n",
    "                return result.get(\"translation_text\", str(result))\n",
    "            else:\n",
    "                return str(result)\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Error en traducción: {str(e)}\"\n",
    "    \n",
    "    def chat_completions_create(self, model, messages):\n",
    "        try:\n",
    "            user_message = \"\"\n",
    "            for msg in reversed(messages):\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    user_message = msg[\"content\"]\n",
    "                    break\n",
    "            \n",
    "            translation = self.translate(user_message)\n",
    "            \n",
    "            class Choice:\n",
    "                def __init__(self, translation):\n",
    "                    self.message = type('obj', (object,), {'content': translation})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, translation):\n",
    "                    self.choices = [Choice(translation)]\n",
    "            \n",
    "            return Response(translation)\n",
    "        except Exception as e:\n",
    "            class Choice:\n",
    "                def __init__(self, error):\n",
    "                    self.message = type('obj', (object,), {'content': f\"Error Helsinki: {error}\"})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, error):\n",
    "                    self.choices = [Choice(error)]\n",
    "            \n",
    "            return Response(str(e))\n",
    "\n",
    "helsinki_model_instance = HelsinkiModel(helsinki_client)\n",
    "\n",
    "helsinki_model = dict(\n",
    "    model=helsinki_model_instance, \n",
    "    model_name=\"Helsinki-NLP/opus-mt-en-es\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1b89d",
   "metadata": {},
   "source": [
    "## Runwayml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "999f25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunwaymlModel:\n",
    "    def __init__(self):\n",
    "        self.model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "        self.pipe = None\n",
    "        self.model_name = \"runwayml/stable-diffusion-v1-5\"\n",
    "        self.initialized = False\n",
    "        self.use_placeholder = False\n",
    "        \n",
    "    def initialize_model(self):\n",
    "        if self.initialized:\n",
    "            return\n",
    "            \n",
    "        print(\"Intentando inicializar Stable Diffusion...\")\n",
    "        \n",
    "        try:\n",
    "            self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "                self.model_id,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                safety_checker=None,\n",
    "                requires_safety_checker=False,\n",
    "                use_safetensors=True\n",
    "            )\n",
    "            \n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            print(f\"Usando dispositivo: {device}\")\n",
    "            \n",
    "            self.pipe = self.pipe.to(device)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                self.pipe.enable_attention_slicing()\n",
    "                try:\n",
    "                    self.pipe.enable_model_cpu_offload()\n",
    "                except:\n",
    "                    print(\"⚠️CPU offload no disponible\")\n",
    "            \n",
    "            print(\"Stable Diffusion inicializado correctamente\")\n",
    "            self.use_placeholder = False\n",
    "            self.initialized = True\n",
    "            \n",
    "        except ImportError as e:\n",
    "            self.use_placeholder = True\n",
    "            self.pipe = \"placeholder\"\n",
    "            self.initialized = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.use_placeholder = True\n",
    "            self.pipe = \"placeholder\"\n",
    "            self.initialized = True\n",
    "        \n",
    "    def generate_image(self, prompt, negative_prompt=\"blurry, low quality, distorted\", num_inference_steps=20, guidance_scale=7.5):\n",
    "        try:\n",
    "            if not self.initialized:\n",
    "                self.initialize_model()\n",
    "            \n",
    "            if self.use_placeholder or self.pipe == \"placeholder\":\n",
    "                return self._generate_placeholder(prompt)\n",
    "            else:\n",
    "                return self._generate_with_diffusion(prompt, negative_prompt, num_inference_steps, guidance_scale)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error en generación, usando placeholder: {e}\")\n",
    "            return self._generate_placeholder(prompt)\n",
    "    \n",
    "    def _generate_with_diffusion(self, prompt, negative_prompt, num_inference_steps, guidance_scale):\n",
    "        try:\n",
    "            import torch            \n",
    "            generation_kwargs = {\n",
    "                \"prompt\": prompt,\n",
    "                \"negative_prompt\": negative_prompt,\n",
    "                \"num_inference_steps\": num_inference_steps,\n",
    "                \"guidance_scale\": guidance_scale,\n",
    "                \"height\": 512,\n",
    "                \"width\": 512,\n",
    "            }\n",
    "            \n",
    "            # Generar imagen\n",
    "            with torch.autocast(\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "                result = self.pipe(**generation_kwargs)\n",
    "                image = result.images[0]\n",
    "            \n",
    "            # Convertir a base64\n",
    "            buffer = io.BytesIO()\n",
    "            image.save(buffer, format='PNG')\n",
    "            img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "            \n",
    "            # Guardar imagen\n",
    "            timestamp = int(time.time())\n",
    "            os.makedirs(\"images\", exist_ok=True)\n",
    "            image_filename = f\"images/stable_diffusion_{timestamp}.png\"\n",
    "            image.save(image_filename)\n",
    "            \n",
    "            print(f\"Imagen generada con Stable Diffusion: {image_filename}\")\n",
    "            return image, img_base64, image_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error con Stable Diffusion, usando placeholder: {e}\")\n",
    "            return self._generate_placeholder(prompt)\n",
    "    \n",
    "    def _generate_placeholder(self, prompt):\n",
    "        \"\"\"Genera una imagen placeholder visual\"\"\"\n",
    "        try:\n",
    "            import hashlib\n",
    "            hash_value = int(hashlib.md5(prompt.encode()).hexdigest()[:6], 16)\n",
    "            \n",
    "            r = (hash_value >> 16) & 255\n",
    "            g = (hash_value >> 8) & 255\n",
    "            b = hash_value & 255\n",
    "            \n",
    "            # Asegurar colores visibles\n",
    "            r = max(50, min(200, r))\n",
    "            g = max(50, min(200, g))\n",
    "            b = max(50, min(200, b))\n",
    "            \n",
    "            img = Image.new('RGB', (512, 512), color=(r, g, b))\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            \n",
    "            try:\n",
    "                words = prompt.split()[:4]\n",
    "                text = \" \".join(words)\n",
    "                \n",
    "                bbox = draw.textbbox((0, 0), text)\n",
    "                text_width = bbox[2] - bbox[0]\n",
    "                text_height = bbox[3] - bbox[1]\n",
    "                \n",
    "                x = (512 - text_width) // 2\n",
    "                y = (512 - text_height) // 2\n",
    "                \n",
    "                for dx in [-1, 0, 1]:\n",
    "                    for dy in [-1, 0, 1]:\n",
    "                        if dx != 0 or dy != 0:\n",
    "                            draw.text((x + dx, y + dy), text, fill=(0, 0, 0))\n",
    "                \n",
    "                draw.text((x, y), text, fill=(255, 255, 255))\n",
    "                \n",
    "                placeholder_text = \"PLACEHOLDER\"\n",
    "                bbox2 = draw.textbbox((0, 0), placeholder_text)\n",
    "                text_width2 = bbox2[2] - bbox2[0]\n",
    "                x2 = (512 - text_width2) // 2\n",
    "                y2 = 450\n",
    "                \n",
    "                draw.text((x2, y2), placeholder_text, fill=(200, 200, 200))\n",
    "                \n",
    "            except Exception as text_error:\n",
    "                print(f\"Error con texto: {text_error}\")\n",
    "                draw.ellipse([200, 200, 312, 312], fill=(255, 255, 255))\n",
    "            \n",
    "            buffer = io.BytesIO()\n",
    "            img.save(buffer, format='PNG')\n",
    "            img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "            \n",
    "            os.makedirs(\"images\", exist_ok=True)\n",
    "            image_filename = f\"images/placeholder_image_{timestamp}.png\"\n",
    "            img.save(image_filename)\n",
    "            \n",
    "            return img, img_base64, image_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generando imagen placeholder: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def chat_completions_create(self, messages):\n",
    "        \"\"\"Interfaz compatible con OpenAI para generación de imágenes\"\"\"\n",
    "        try:\n",
    "            user_message = \"\"\n",
    "            for msg in reversed(messages):\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    user_message = msg[\"content\"]\n",
    "                    break\n",
    "            \n",
    "            if not user_message:\n",
    "                raise Exception(\"No se encontró prompt para generar imagen\")\n",
    "            \n",
    "            image, img_base64, filename = self.generate_image(user_message)\n",
    "            \n",
    "            if self.use_placeholder or self.pipe == \"placeholder\":\n",
    "                response_content = f\"![Imagen generada](data:image/png;base64,{img_base64})\\n\"\n",
    "            else:\n",
    "                response_content = f\"![Imagen generada](data:image/png;base64,{img_base64})\\n\"\n",
    "            \n",
    "            class Choice:\n",
    "                def __init__(self, content):\n",
    "                    self.message = type('obj', (object,), {'content': content})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, content):\n",
    "                    self.choices = [Choice(content)]\n",
    "            \n",
    "            return Response(response_content)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"**Error generando imagen:** {str(e)}\"\n",
    "            print(error_msg)\n",
    "            \n",
    "            class Choice:\n",
    "                def __init__(self, error):\n",
    "                    self.message = type('obj', (object,), {'content': error})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, error):\n",
    "                    self.choices = [Choice(error)]\n",
    "            \n",
    "            return Response(error_msg)\n",
    "\n",
    "runwayml_model_instance = RunwaymlModel()\n",
    "\n",
    "runwayml_model = dict(\n",
    "    model=runwayml_model_instance, \n",
    "    model_name=\"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca0c5d",
   "metadata": {},
   "source": [
    "### stabilityai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f80257e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo gratuito de Hugging Face para generación de imágenes\n",
    "class HuggingFaceImageModel:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.model_name = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "        \n",
    "    def generate_image_hf(self, prompt):\n",
    "        try:\n",
    "            # Usar el cliente de Hugging Face para generar imagen\n",
    "            image = self.client.text_to_image(\n",
    "                prompt=prompt,\n",
    "                model=self.model_name\n",
    "            )\n",
    "            \n",
    "            # La imagen ya es un objeto PIL, no necesita conversión desde bytes\n",
    "            # Convertir a base64\n",
    "            buffer = io.BytesIO()\n",
    "            image.save(buffer, format='PNG')\n",
    "            img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "            \n",
    "            # Guardar imagen\n",
    "            timestamp = int(time.time())\n",
    "            os.makedirs(\"images\", exist_ok=True)\n",
    "            image_filename = f\"images/hf_sdxl_{timestamp}.png\"\n",
    "            image.save(image_filename)\n",
    "            \n",
    "            print(f\"Imagen generada con Hugging Face SDXL: {image_filename}\")\n",
    "            return image, img_base64, image_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error con Hugging Face, generando placeholder: {e}\")\n",
    "            return self._generate_placeholder(prompt)\n",
    "    \n",
    "    def _generate_placeholder(self, prompt):\n",
    "        \"\"\"Genera una imagen placeholder visual\"\"\"\n",
    "        try:\n",
    "            import hashlib\n",
    "            hash_value = int(hashlib.md5(prompt.encode()).hexdigest()[:6], 16)\n",
    "            \n",
    "            r = (hash_value >> 16) & 255\n",
    "            g = (hash_value >> 8) & 255\n",
    "            b = hash_value & 255\n",
    "            \n",
    "            r = max(50, min(200, r))\n",
    "            g = max(50, min(200, g))\n",
    "            b = max(50, min(200, b))\n",
    "            \n",
    "            img = Image.new('RGB', (512, 512), color=(r, g, b))\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            \n",
    "            try:\n",
    "                words = prompt.split()[:4]\n",
    "                text = \" \".join(words)\n",
    "                \n",
    "                bbox = draw.textbbox((0, 0), text)\n",
    "                text_width = bbox[2] - bbox[0]\n",
    "                text_height = bbox[3] - bbox[1]\n",
    "                \n",
    "                x = (512 - text_width) // 2\n",
    "                y = (512 - text_height) // 2\n",
    "                \n",
    "                for dx in [-1, 0, 1]:\n",
    "                    for dy in [-1, 0, 1]:\n",
    "                        if dx != 0 or dy != 0:\n",
    "                            draw.text((x + dx, y + dy), text, fill=(0, 0, 0))\n",
    "                \n",
    "                draw.text((x, y), text, fill=(255, 255, 255))\n",
    "                \n",
    "                placeholder_text = \"HF PLACEHOLDER\"\n",
    "                bbox2 = draw.textbbox((0, 0), placeholder_text)\n",
    "                text_width2 = bbox2[2] - bbox2[0]\n",
    "                x2 = (512 - text_width2) // 2\n",
    "                y2 = 450\n",
    "                \n",
    "                draw.text((x2, y2), placeholder_text, fill=(200, 200, 200))\n",
    "                \n",
    "            except Exception as text_error:\n",
    "                print(f\"Error con texto: {text_error}\")\n",
    "                draw.ellipse([200, 200, 312, 312], fill=(255, 255, 255))\n",
    "            \n",
    "            buffer = io.BytesIO()\n",
    "            img.save(buffer, format='PNG')\n",
    "            img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "            \n",
    "            os.makedirs(\"images\", exist_ok=True)\n",
    "            image_filename = f\"images/placeholder_hf_{timestamp}.png\"\n",
    "            img.save(image_filename)\n",
    "            \n",
    "            return img, img_base64, image_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generando placeholder: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def chat_completions_create(self, messages):\n",
    "        \"\"\"Interfaz compatible con OpenAI para generación de imágenes\"\"\"\n",
    "        try:\n",
    "            user_message = \"\"\n",
    "            for msg in reversed(messages):\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    user_message = msg[\"content\"]\n",
    "                    break\n",
    "            \n",
    "            if not user_message:\n",
    "                raise Exception(\"No se encontró prompt para generar imagen\")\n",
    "            \n",
    "            image, img_base64, filename = self.generate_image_hf(user_message)\n",
    "            \n",
    "            response_content = f\"![Imagen generada](data:image/png;base64,{img_base64})\\n**Generado con Stable Diffusion XL (Hugging Face)**\"\n",
    "            \n",
    "            class Choice:\n",
    "                def __init__(self, content):\n",
    "                    self.message = type('obj', (object,), {'content': content})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, content):\n",
    "                    self.choices = [Choice(content)]\n",
    "            \n",
    "            return Response(response_content)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"**Error generando imagen:** {str(e)}\"\n",
    "            print(error_msg)\n",
    "            \n",
    "            class Choice:\n",
    "                def __init__(self, error):\n",
    "                    self.message = type('obj', (object,), {'content': error})()\n",
    "            \n",
    "            class Response:\n",
    "                def __init__(self, error):\n",
    "                    self.choices = [Choice(error)]\n",
    "            \n",
    "            return Response(error_msg)\n",
    "\n",
    "hf_image_model_instance = HuggingFaceImageModel(helsinki_client)\n",
    "\n",
    "hf_image_model = dict(\n",
    "    model=hf_image_model_instance,\n",
    "    model_name=\"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f01a9",
   "metadata": {},
   "source": [
    "## OpenRouterAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "670cfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_router_model = dict(model=OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=open_router_api_key,\n",
    "), \n",
    "model_name=\"deepseek/deepseek-r1-0528\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8d7ae",
   "metadata": {},
   "source": [
    "## Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1fd904f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_model = dict(model=OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\"), model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cfcfc0",
   "metadata": {},
   "source": [
    "# Crear interfaz de chat con gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c15b1a",
   "metadata": {},
   "source": [
    "## Funcion de chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a9f1a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_data = {\n",
    "    \"total_requests\": 0,\n",
    "    \"successful_requests\": 0,\n",
    "    \"failed_requests\": 0,\n",
    "    \"requests_by_model\": {},\n",
    "    \"requests_by_task\": {},\n",
    "    \"response_times\": [],\n",
    "    \"errors\": [],\n",
    "    \"session_start\": datetime.now().isoformat(),\n",
    "    \"last_request\": None,\n",
    "    \"images_generated\": 0,\n",
    "    \"translations_made\": 0\n",
    "}\n",
    "\n",
    "def log_metric(metric_type, **kwargs):\n",
    "    \"\"\"Registra métricas del sistema\"\"\"\n",
    "    global metrics_data\n",
    "    \n",
    "    try:\n",
    "        current_time = datetime.now().isoformat()\n",
    "        \n",
    "        if metric_type == \"request_start\":\n",
    "            metrics_data[\"total_requests\"] += 1\n",
    "            metrics_data[\"last_request\"] = current_time\n",
    "            \n",
    "            # Contar por modelo\n",
    "            model = kwargs.get(\"model\", \"unknown\")\n",
    "            if model not in metrics_data[\"requests_by_model\"]:\n",
    "                metrics_data[\"requests_by_model\"][model] = 0\n",
    "            metrics_data[\"requests_by_model\"][model] += 1\n",
    "            \n",
    "            # Contar por tarea\n",
    "            task = kwargs.get(\"task\", \"unknown\")\n",
    "            if task not in metrics_data[\"requests_by_task\"]:\n",
    "                metrics_data[\"requests_by_task\"][task] = 0\n",
    "            metrics_data[\"requests_by_task\"][task] += 1\n",
    "            \n",
    "        elif metric_type == \"request_success\":\n",
    "            metrics_data[\"successful_requests\"] += 1\n",
    "            response_time = kwargs.get(\"response_time\", 0)\n",
    "            metrics_data[\"response_times\"].append(response_time)\n",
    "            \n",
    "            # Contar tipos específicos\n",
    "            task = kwargs.get(\"task\", \"\")\n",
    "            if task == \"imagenes\":\n",
    "                metrics_data[\"images_generated\"] += 1\n",
    "            elif task == \"traduccion\":\n",
    "                metrics_data[\"translations_made\"] += 1\n",
    "                \n",
    "        elif metric_type == \"request_error\":\n",
    "            metrics_data[\"failed_requests\"] += 1\n",
    "            error_info = {\n",
    "                \"timestamp\": current_time,\n",
    "                \"model\": kwargs.get(\"model\", \"unknown\"),\n",
    "                \"task\": kwargs.get(\"task\", \"unknown\"),\n",
    "                \"error\": kwargs.get(\"error\", \"Unknown error\"),\n",
    "                \"user_message\": kwargs.get(\"user_message\", \"\")[:100]  # Primeros 100 chars\n",
    "            }\n",
    "            metrics_data[\"errors\"].append(error_info)\n",
    "            \n",
    "            # Mantener solo los últimos 50 errores para no usar demasiada memoria\n",
    "            if len(metrics_data[\"errors\"]) > 50:\n",
    "                metrics_data[\"errors\"] = metrics_data[\"errors\"][-50:]\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error registrando métrica: {e}\")\n",
    "\n",
    "def get_metrics_summary():\n",
    "    try:\n",
    "        response_times = metrics_data[\"response_times\"]\n",
    "        if response_times:\n",
    "            avg_response_time = sum(response_times) / len(response_times)\n",
    "            min_response_time = min(response_times)\n",
    "            max_response_time = max(response_times)\n",
    "        else:\n",
    "            avg_response_time = min_response_time = max_response_time = 0\n",
    "        \n",
    "        # Calcular tasa de éxito\n",
    "        total = metrics_data[\"total_requests\"]\n",
    "        success_rate = (metrics_data[\"successful_requests\"] / total * 100) if total > 0 else 0\n",
    "        \n",
    "        # Tiempo de sesión\n",
    "        session_start = datetime.fromisoformat(metrics_data[\"session_start\"])\n",
    "        session_duration = datetime.now() - session_start\n",
    "        \n",
    "        # Modelo más usado\n",
    "        model_usage = metrics_data[\"requests_by_model\"]\n",
    "        most_used_model = max(model_usage.items(), key=lambda x: x[1]) if model_usage else (\"Ninguno\", 0)\n",
    "        \n",
    "        # Tarea más usada\n",
    "        task_usage = metrics_data[\"requests_by_task\"]\n",
    "        most_used_task = max(task_usage.items(), key=lambda x: x[1]) if task_usage else (\"Ninguna\", 0)\n",
    "        \n",
    "        summary = f\"\"\"#**Métricas del ChatBot**\n",
    "\n",
    "##**Estadísticas Generales**\n",
    "- **Total de solicitudes:** {total}\n",
    "- **Solicitudes exitosas:** {metrics_data[\"successful_requests\"]} ({success_rate:.1f}%)\n",
    "- **Solicitudes fallidas:** {metrics_data[\"failed_requests\"]}\n",
    "- **Imágenes generadas:** {metrics_data[\"images_generated\"]}\n",
    "- **Traducciones realizadas:** {metrics_data[\"translations_made\"]}\n",
    "\n",
    "##**Tiempos de Respuesta**\n",
    "- **Promedio:** {avg_response_time:.2f}s\n",
    "- **Mínimo:** {min_response_time:.2f}s\n",
    "- **Máximo:** {max_response_time:.2f}s\n",
    "- **Total de mediciones:** {len(response_times)}\n",
    "\n",
    "##**Uso por Modelo**\n",
    "\"\"\"\n",
    "        \n",
    "        # Agregar uso por modelo\n",
    "        for model, count in sorted(model_usage.items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / total * 100) if total > 0 else 0\n",
    "            summary += f\"- **{model.capitalize()}:** {count} ({percentage:.1f}%)\\n\"\n",
    "        \n",
    "        summary += f\"\\n## 📋 **Uso por Tarea**\\n\"\n",
    "        \n",
    "        # Agregar uso por tarea\n",
    "        for task, count in sorted(task_usage.items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / total * 100) if total > 0 else 0\n",
    "            summary += f\"- **{task.capitalize()}:** {count} ({percentage:.1f}%)\\n\"\n",
    "        \n",
    "        summary += f\"\"\"\n",
    "##**Información de Sesión**\n",
    "- **Inicio de sesión:** {session_start.strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "- **Duración:** {str(session_duration).split('.')[0]}\n",
    "- **Última solicitud:** {metrics_data[\"last_request\"] or \"Ninguna\"}\n",
    "\n",
    "##**Más Utilizados**\n",
    "- **Modelo más usado:** {most_used_model[0]} ({most_used_model[1]} usos)\n",
    "- **Tarea más usada:** {most_used_task[0]} ({most_used_task[1]} usos)\n",
    "\"\"\"\n",
    "        \n",
    "        # Agregar errores recientes si los hay\n",
    "        recent_errors = metrics_data[\"errors\"][-5:] if metrics_data[\"errors\"] else []\n",
    "        if recent_errors:\n",
    "            summary += \"\\n## ⚠️ **Errores Recientes (Últimos 5)**\\n\"\n",
    "            for i, error in enumerate(recent_errors, 1):\n",
    "                timestamp = error[\"timestamp\"][:19].replace(\"T\", \" \")  # Formato legible\n",
    "                summary += f\"{i}. **[{timestamp}]** {error['model']} - {error['task']}: {error['error'][:100]}...\\n\"\n",
    "        \n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"**Error generando métricas:** {str(e)}\"\n",
    "\n",
    "def reset_metrics():\n",
    "    global metrics_data\n",
    "    metrics_data = {\n",
    "        \"total_requests\": 0,\n",
    "        \"successful_requests\": 0,\n",
    "        \"failed_requests\": 0,\n",
    "        \"requests_by_model\": {},\n",
    "        \"requests_by_task\": {},\n",
    "        \"response_times\": [],\n",
    "        \"errors\": [],\n",
    "        \"session_start\": datetime.now().isoformat(),\n",
    "        \"last_request\": None,\n",
    "        \"images_generated\": 0,\n",
    "        \"translations_made\": 0\n",
    "    }\n",
    "    return \"**Métricas reiniciadas correctamente**\"\n",
    "\n",
    "def obtener_valor(dict: dict, key):\n",
    "    return dict.get(key, \"Clave no encontrada\")\n",
    "\n",
    "models = dict(\n",
    "    gemini=gemini_model,\n",
    "    open_router=open_router_model,\n",
    "    groq=groq_model,\n",
    "    helsinki=helsinki_model,  \n",
    "    runwayml=runwayml_model,\n",
    "    hf_image=hf_image_model\n",
    ")\n",
    "\n",
    "tasks_config = {\n",
    "    \"traduccion\": {\n",
    "        \"models\": [\"gemini\", \"helsinki\"], \n",
    "        \"description\": \"Traducir texto entre idiomas (Helsinki: solo inglés → español)\"\n",
    "    },\n",
    "    \"resumen\": {\n",
    "        \"models\": [\"groq\", \"open_router\"],\n",
    "        \"description\": \"Resumir texto o documentos\"\n",
    "    },\n",
    "    \"imagenes\": {\n",
    "        \"models\": [\"runwayml\", \"stabilityai\"],  \n",
    "        \"description\": \"Generar imágenes usando Stable Diffusion local o Hugging Face SDXL (gratis)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_available_models(task):\n",
    "    \"\"\"Obtiene los modelos disponibles para una tarea específica\"\"\"\n",
    "    return tasks_config.get(task, {}).get(\"models\", [])\n",
    "\n",
    "def validate_input(message):\n",
    "    \"\"\"Valida el mensaje de entrada\"\"\"\n",
    "    if not message:\n",
    "        return False, \"**Error**: No puedes enviar un mensaje vacío. Por favor, escribe algo.\"\n",
    "    \n",
    "    if len(message.strip()) == 0:\n",
    "        return False, \"**Error**: El mensaje solo contiene espacios en blanco. Por favor, escribe un mensaje válido.\"\n",
    "    \n",
    "    if len(message) > 8000:\n",
    "        return False, \"**Error**: El mensaje es demasiado largo (máximo 8000 caracteres). Por favor, acórtalo.\"\n",
    "    \n",
    "    return True, None\n",
    "\n",
    "def format_inference_time(seconds):\n",
    "    if seconds < 1:\n",
    "        return f\"{seconds*1000:.0f}ms\"\n",
    "    elif seconds < 60:\n",
    "        return f\"{seconds:.2f}s\"\n",
    "    else:\n",
    "        minutes = int(seconds // 60)\n",
    "        remaining_seconds = seconds % 60\n",
    "        return f\"{minutes}m {remaining_seconds:.1f}s\"\n",
    "\n",
    "def create_initial_chat():\n",
    "    chat_id = f\"Chat-{str(uuid.uuid4())[:8]}\"\n",
    "    return chat_id, []\n",
    "\n",
    "def chat(message, history, model, task, chats_state, current_chat_id):    \n",
    "    total_start_time = time.time()\n",
    "    log_metric(\"request_start\", model=model, task=task, user_message=message)\n",
    "    is_valid, error_msg = validate_input(message)\n",
    "    if not is_valid:\n",
    "        log_metric(\"request_error\", model=model, task=task, error=\"Invalid input\", user_message=message)\n",
    "        return error_msg\n",
    "    try:\n",
    "        print(f\"Tarea seleccionada: {task}\")\n",
    "        print(f\"Modelo seleccionado: {model}\")\n",
    "        print(f\"Chat actual: {current_chat_id}\")\n",
    "        if task not in tasks_config:\n",
    "            error = f\"Tarea '{task}' no reconocida\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"❌ **Error**: {error}. Tareas disponibles: {', '.join(tasks_config.keys())}\"\n",
    "        \n",
    "        available_models = get_available_models(task)\n",
    "        if not available_models:\n",
    "            error = f\"No hay modelos disponibles para la tarea '{task}'\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"❌ **Error**: {error}.\"\n",
    "        \n",
    "        if model not in available_models:\n",
    "            error = f\"El modelo '{model}' no está disponible para la tarea '{task}'\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"❌ **Error**: {error}. Modelos disponibles: {', '.join(available_models)}\"\n",
    "        \n",
    "        if model not in models:\n",
    "            error = f\"El modelo '{model}' no está configurado en el sistema\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"❌ **Error**: {error}.\"\n",
    "        \n",
    "        model_config = models[model]\n",
    "        if not model_config or obtener_valor(model_config, \"model\") == \"Clave no encontrada\":\n",
    "            error = f\"El modelo '{model}' no está disponible. Verifica la configuración de API keys\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"❌ **Error**: {error}.\"\n",
    "\n",
    "        print(f\"modelo: {obtener_valor(model_config, 'model_name')}\")\n",
    "\n",
    "        system_prompt = \"Eres un asistente de IA que responde preguntas y ayuda con tareas.\"\n",
    "        \n",
    "        if task == \"traduccion\":\n",
    "            if model == \"helsinki\":\n",
    "                system_prompt = \"Eres un traductor especializado que traduce ÚNICAMENTE de inglés a español usando modelos Helsinki-NLP. Solo acepta texto en inglés y lo traduce al español.\"\n",
    "            else:\n",
    "                system_prompt = \"Eres un traductor experto. Tu tarea es traducir texto entre diferentes idiomas de manera precisa y natural.\"\n",
    "        elif task == \"resumen\":\n",
    "            system_prompt = \"Eres un asistente especializado en resumir texto. Tu tarea es crear resúmenes claros y concisos del contenido proporcionado.\"\n",
    "        elif task == \"imagenes\":\n",
    "            if model == \"runwayml\":\n",
    "                system_prompt = \"Eres un generador de imágenes que usa Stable Diffusion cuando está disponible, o genera placeholders visuales cuando hay problemas técnicos. Conviertes descripciones de texto en imágenes.\"\n",
    "\n",
    "        chat_history = []\n",
    "        if current_chat_id and current_chat_id in chats_state:\n",
    "            chat_history = chats_state[current_chat_id].copy()\n",
    "        \n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        \n",
    "        recent_history = chat_history[-20:] if len(chat_history) > 20 else chat_history\n",
    "        for msg in recent_history:\n",
    "            if isinstance(msg, dict) and \"role\" in msg and \"content\" in msg:\n",
    "                messages.append({\"role\": msg[\"role\"], \"content\": msg[\"content\"]})\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        model_instance = obtener_valor(model_config, \"model\")\n",
    "        response_content = None\n",
    "        \n",
    "        inference_start_time = None\n",
    "        inference_end_time = None\n",
    "        total_attempts = 0\n",
    "        \n",
    "        max_retries = 2\n",
    "        for attempt in range(max_retries + 1):\n",
    "            try:\n",
    "                total_attempts += 1\n",
    "                \n",
    "                inference_start_time = time.time()\n",
    "                if model in [\"helsinki\", \"runwayml\", \"hf_image\"]:\n",
    "                    resp = model_instance.chat_completions_create(\n",
    "                        messages=messages,\n",
    "                    )\n",
    "                else:\n",
    "                    resp = model_instance.chat.completions.create(\n",
    "                        model=obtener_valor(model_config, \"model_name\"),\n",
    "                        messages=messages,\n",
    "                    )\n",
    "\n",
    "                inference_end_time = time.time()\n",
    "\n",
    "                if not resp or not hasattr(resp, 'choices') or len(resp.choices) == 0:\n",
    "                    raise Exception(\"Respuesta vacía del modelo\")\n",
    "                \n",
    "                if not hasattr(resp.choices[0], 'message') or not hasattr(resp.choices[0].message, 'content'):\n",
    "                    raise Exception(\"Formato de respuesta inválido\")\n",
    "                \n",
    "                response_content = resp.choices[0].message.content\n",
    "                \n",
    "                if not response_content or len(response_content.strip()) == 0:\n",
    "                    raise Exception(\"El modelo devolvió una respuesta vacía\")\n",
    "                \n",
    "                break \n",
    "                \n",
    "            except Timeout:\n",
    "                inference_end_time = time.time()  \n",
    "                if attempt < max_retries:\n",
    "                    print(f\"Timeout en intento {attempt + 1}, reintentando...\")\n",
    "                    time.sleep(2)  \n",
    "                    continue\n",
    "                else:\n",
    "                    error = f\"El modelo '{model}' tardó demasiado en responder\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"⏱**Error de Timeout**: {error}. Intenta de nuevo en unos momentos.\"\n",
    "            \n",
    "            except ConnectionError:\n",
    "                inference_end_time = time.time()  \n",
    "                if attempt < max_retries:\n",
    "                    print(f\"Error de conexión en intento {attempt + 1}, reintentando...\")\n",
    "                    time.sleep(2)\n",
    "                    continue\n",
    "                else:\n",
    "                    error = f\"No se pudo conectar con el modelo '{model}'\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"**Error de Conexión**: {error}. Verifica tu conexión a internet.\"\n",
    "            \n",
    "            except Exception as api_error:\n",
    "                inference_end_time = time.time()\n",
    "                error_msg = str(api_error).lower()\n",
    "                \n",
    "                if \"rate limit\" in error_msg or \"quota\" in error_msg:\n",
    "                    error = f\"Límite de uso alcanzado para el modelo '{model}'\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"**Límite de Uso**: {error}. Intenta más tarde o cambia de modelo.\"\n",
    "                \n",
    "                elif \"authentication\" in error_msg or \"unauthorized\" in error_msg or \"api key\" in error_msg:\n",
    "                    error = f\"Error de autenticación del modelo '{model}'\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"**Error de Autenticación**: La API key del modelo '{model}' es inválida o ha expirado. Verifica tu configuración.\"\n",
    "                \n",
    "                elif \"not found\" in error_msg or \"404\" in error_msg:\n",
    "                    error = f\"El modelo '{obtener_valor(model_config, 'model_name')}' no está disponible\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"**Modelo No Encontrado**: {error} en este momento.\"\n",
    "                \n",
    "                elif \"server error\" in error_msg or \"500\" in error_msg or \"502\" in error_msg or \"503\" in error_msg:\n",
    "                    if attempt < max_retries:\n",
    "                        print(f\"Error del servidor en intento {attempt + 1}, reintentando...\")\n",
    "                        time.sleep(3)\n",
    "                        continue\n",
    "                    else:\n",
    "                        error = f\"El servicio del modelo '{model}' está temporalmente no disponible\"\n",
    "                        log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                        return f\"🔧 **Error del Servidor**: {error}. Intenta más tarde.\"\n",
    "                \n",
    "                elif \"content policy\" in error_msg or \"safety\" in error_msg:\n",
    "                    error = f\"Contenido bloqueado por las políticas de seguridad del modelo '{model}'\"\n",
    "                    log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                    return f\"⚠️**Contenido Bloqueado**: {error}. Reformula tu pregunta.\"\n",
    "                \n",
    "                else:\n",
    "                    if attempt < max_retries:\n",
    "                        print(f\"Error genérico en intento {attempt + 1}: {api_error}\")\n",
    "                        time.sleep(2)\n",
    "                        continue\n",
    "                    else:\n",
    "                        error = f\"Problema con el modelo '{model}': {str(api_error)[:100]}\"\n",
    "                        log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "                        return f\"**Error**: {error}...\"\n",
    "\n",
    "        if not response_content:\n",
    "            error = f\"No se pudo obtener una respuesta válida del modelo '{model}' después de {max_retries + 1} intentos\"\n",
    "            log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "            return f\"**Error**: {error}.\"\n",
    "        \n",
    "        total_end_time = time.time()\n",
    "        total_time = total_end_time - total_start_time\n",
    "        \n",
    "        if inference_start_time and inference_end_time:\n",
    "            inference_time = inference_end_time - inference_start_time\n",
    "        else:\n",
    "            inference_time = total_time\n",
    "        \n",
    "        log_metric(\"request_success\", model=model, task=task, response_time=inference_time)\n",
    "        \n",
    "        model_name = obtener_valor(model_config, \"model_name\")\n",
    "        model_display = f\"{model.capitalize()}\"\n",
    "        if model_name and model_name != \"Clave no encontrada\":\n",
    "            model_display += f\" ({model_name})\"\n",
    "        \n",
    "        time_metadata = f\"<small style='color: #888; font-size: 0.85em;'>⚡ {format_inference_time(inference_time)}\"\n",
    "        \n",
    "        if total_attempts > 1:\n",
    "            time_metadata += f\" ({total_attempts}º intento)\"\n",
    "        \n",
    "        time_metadata += f\" • {model_display}</small>\"\n",
    "        \n",
    "        final_response = response_content + \"\\n\\n\" + time_metadata\n",
    "        \n",
    "        if current_chat_id:\n",
    "            try:\n",
    "                current_history = chats_state.get(current_chat_id, []).copy()\n",
    "                \n",
    "                current_history.extend([\n",
    "                    {\"role\": \"user\", \"content\": message},\n",
    "                    {\"role\": \"assistant\", \"content\": final_response}\n",
    "                ])\n",
    "                \n",
    "                chats_state[current_chat_id] = current_history\n",
    "                \n",
    "                print(f\"Historial guardado en chat {current_chat_id}: {len(current_history)} mensajes\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al guardar en historial: {e}\")        \n",
    "        return final_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        total_end_time = time.time()\n",
    "        total_time = total_end_time - total_start_time\n",
    "        error = f\"Error inesperado: {str(e)[:200]}\"\n",
    "        log_metric(\"request_error\", model=model, task=task, error=error, user_message=message)\n",
    "        print(f\"Error inesperado: {e}\")\n",
    "        return f\"❌ **Error Inesperado**: Ocurrió un problema técnico: {str(e)[:200]}... \\n\\n<small style='color: #888;'>⏱️ {format_inference_time(total_time)}</small>\"\n",
    "    \n",
    "def new_chat(chats_state: dict):\n",
    "    try:\n",
    "        chat_id = f\"Chat-{str(uuid.uuid4())[:8]}\" \n",
    "        while chat_id in chats_state:\n",
    "            chat_id = f\"Chat-{str(uuid.uuid4())[:8]}\"\n",
    "        chats_state[chat_id] = []\n",
    "        print(f\"Nuevo chat creado: {chat_id}\")\n",
    "        chat_choices = list(chats_state.keys())\n",
    "\n",
    "        return chats_state, chat_id, gr.Dropdown(choices=chat_choices, value=chat_id), []\n",
    "    except Exception as e:\n",
    "        print(f\"Error al crear nuevo chat: {e}\")\n",
    "        return chats_state, \"\", gr.Dropdown(choices=list(chats_state.keys())), []\n",
    "\n",
    "def switch_chat(chats_state, chat_id):\n",
    "    try:\n",
    "        print(f\"Cambiando a chat: {chat_id}\")\n",
    "        \n",
    "        if chat_id and chat_id in chats_state:\n",
    "            chat_history = chats_state[chat_id].copy()\n",
    "            print(f\"Historial del chat {chat_id}: {len(chat_history)} mensajes\")\n",
    "            return chat_history\n",
    "        else:\n",
    "            print(f\"Chat {chat_id} no encontrado o ID vacío\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cambiar chat: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_message(chats_state, chat_id, history):\n",
    "    try:\n",
    "        if chat_id and chat_id in chats_state:\n",
    "            print(f\"save_message llamado para {chat_id} - ignorando para evitar conflictos\")\n",
    "        return chats_state\n",
    "    except Exception as e:\n",
    "        print(f\"Error en save_message: {e}\")\n",
    "        return chats_state\n",
    "\n",
    "def initialize_first_chat():\n",
    "    chat_id, empty_history = create_initial_chat()\n",
    "    initial_state = {chat_id: empty_history}\n",
    "    return initial_state, chat_id, [chat_id], empty_history\n",
    "\n",
    "def update_models_for_task(task):\n",
    "    try:\n",
    "        available_models = get_available_models(task)\n",
    "        if available_models:\n",
    "            return gr.Dropdown(choices=available_models, value=available_models[0])\n",
    "        else:\n",
    "            return gr.Dropdown(choices=[], value=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al actualizar modelos: {e}\")\n",
    "        return gr.Dropdown(choices=[], value=None)\n",
    "\n",
    "def update_chat_list(chats_state):\n",
    "    try:\n",
    "        chat_choices = list(chats_state.keys()) if chats_state else []\n",
    "        return gr.Dropdown(choices=chat_choices)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al actualizar lista de chats: {e}\")\n",
    "        return gr.Dropdown(choices=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206937d",
   "metadata": {},
   "source": [
    "## Creacion de la interfaz del Chat con Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3dc87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarea seleccionada: imagenes\n",
      "Modelo seleccionado: runwayml\n",
      "Chat actual: Chat-21b744a2\n",
      "modelo: runwayml/stable-diffusion-v1-5\n",
      "Intentando inicializar Stable Diffusion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  83%|████████▎ | 5/6 [00:01<00:00,  2.83it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generando imagen placeholder: name 'timestamp' is not defined\n",
      "Error en generación, usando placeholder: name 'timestamp' is not defined\n",
      "Error generando imagen placeholder: name 'timestamp' is not defined\n",
      "**Error generando imagen:** name 'timestamp' is not defined\n",
      "Historial guardado en chat Chat-21b744a2: 2 mensajes\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as app:\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"## Configuración del Chat\")\n",
    "            \n",
    "            task_selector = gr.Dropdown(\n",
    "                choices=list(tasks_config.keys()),\n",
    "                value=\"traduccion\",\n",
    "                label=\"Tipo de tarea\"\n",
    "            )\n",
    "            \n",
    "            model_selector = gr.Dropdown(\n",
    "                choices=get_available_models(\"traduccion\"),\n",
    "                value=\"gemini\" if get_available_models(\"traduccion\") else None,\n",
    "                label=\"Modelo de IA\"\n",
    "            )\n",
    "\n",
    "            initial_state, initial_chat_id, initial_choices, initial_history = initialize_first_chat()\n",
    "\n",
    "            selected_chat = gr.Dropdown(\n",
    "                choices=initial_choices,\n",
    "                value=initial_chat_id,\n",
    "                label=\"Historial de chats\",\n",
    "                interactive=True\n",
    "            )\n",
    "            \n",
    "            new_chat_btn = gr.Button(\"➕ Nuevo Chat\", size=\"md\")\n",
    "            \n",
    "            task_info = gr.Markdown(f\"**Descripción:** {tasks_config['traduccion']['description']}\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                metrics_btn = gr.Button(\"📊 Ver Métricas\", size=\"sm\", variant=\"secondary\")\n",
    "                reset_metrics_btn = gr.Button(\"🔄 Reiniciar Métricas\", size=\"sm\", variant=\"secondary\")\n",
    "            \n",
    "            metrics_display = gr.Markdown(\n",
    "                value=\"Haz clic en **Ver Métricas** para mostrar las estadísticas de uso.\",\n",
    "                visible=False,\n",
    "                label=\"Métricas del Sistema\"\n",
    "            )\n",
    "            \n",
    "            metrics_visible = gr.State(False)\n",
    "            \n",
    "            chat_state = gr.State(initial_state) \n",
    "            current_chat = gr.State(initial_chat_id) \n",
    "\n",
    "        with gr.Column(scale=3):\n",
    "            current_model = gr.State(\"gemini\")\n",
    "            current_task = gr.State(\"traduccion\")\n",
    "\n",
    "            task_selector.change(\n",
    "                fn=lambda task: [\n",
    "                    update_models_for_task(task),\n",
    "                    f\"**Descripción:** {tasks_config.get(task, {}).get('description', '')}\", \n",
    "                    task,\n",
    "                    get_available_models(task)[0] if get_available_models(task) else None\n",
    "                ],\n",
    "                inputs=task_selector,\n",
    "                outputs=[model_selector, task_info, current_task, current_model]\n",
    "            )\n",
    "\n",
    "            model_selector.change(\n",
    "                fn=lambda model: model,\n",
    "                inputs=model_selector,\n",
    "                outputs=current_model\n",
    "            )\n",
    "\n",
    "            chatbot = gr.Chatbot(type=\"messages\", height=500, value=initial_history)\n",
    "\n",
    "            chat_interface = gr.ChatInterface(\n",
    "                chat, \n",
    "                chatbot=chatbot, \n",
    "                additional_inputs=[current_model, current_task, chat_state, current_chat], \n",
    "                type=\"messages\", \n",
    "                title=\"Chat con Modelos de IA\"\n",
    "            )\n",
    "\n",
    "            def on_chat_change(chats_state, chat_id):                \n",
    "                if not chat_id or chat_id not in chats_state:\n",
    "                    print(f\"Chat {chat_id} no válido\")\n",
    "                    return [], chat_id\n",
    "                chat_history = switch_chat(chats_state, chat_id)                \n",
    "                return chat_history, chat_id\n",
    "\n",
    "            selected_chat.change(\n",
    "                fn=on_chat_change,\n",
    "                inputs=[chat_state, selected_chat],\n",
    "                outputs=[chatbot, current_chat]\n",
    "            )\n",
    "\n",
    "            def on_new_chat(chats_state):                \n",
    "                try:\n",
    "                    new_state, new_chat_id, updated_dropdown, empty_history = new_chat(chats_state)\n",
    "                    \n",
    "                    print(f\"Nuevo chat creado: {new_chat_id}\")\n",
    "                    print(f\"Estado actual: {list(new_state.keys())}\")\n",
    "                    \n",
    "                    return new_state, new_chat_id, updated_dropdown, empty_history\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error al crear nuevo chat: {e}\")\n",
    "                    return chats_state, \"\", gr.Dropdown(choices=list(chats_state.keys())), []\n",
    "\n",
    "            # Función para toggle de métricas\n",
    "            def toggle_metrics(is_visible):\n",
    "                if is_visible:\n",
    "                    return False, gr.Markdown(visible=False)\n",
    "                else:\n",
    "                    metrics_content = get_metrics_summary()\n",
    "                    return True, gr.Markdown(value=metrics_content, visible=True)\n",
    "\n",
    "            # Función para reiniciar métricas\n",
    "            def reset_metrics_display(is_visible):\n",
    "                result = reset_metrics()\n",
    "                if is_visible:\n",
    "                    return is_visible, gr.Markdown(value=result, visible=True)\n",
    "                else:\n",
    "                    return is_visible, gr.Markdown(visible=False)\n",
    "\n",
    "            new_chat_btn.click(\n",
    "                fn=on_new_chat,\n",
    "                inputs=[chat_state],\n",
    "                outputs=[chat_state, current_chat, selected_chat, chatbot]\n",
    "            )\n",
    "\n",
    "            metrics_btn.click(\n",
    "                fn=toggle_metrics,\n",
    "                inputs=metrics_visible,\n",
    "                outputs=[metrics_visible, metrics_display]\n",
    "            )\n",
    "\n",
    "            reset_metrics_btn.click(\n",
    "                fn=reset_metrics_display,\n",
    "                inputs=metrics_visible,\n",
    "                outputs=[metrics_visible, metrics_display]\n",
    "            )\n",
    "\n",
    "app.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
